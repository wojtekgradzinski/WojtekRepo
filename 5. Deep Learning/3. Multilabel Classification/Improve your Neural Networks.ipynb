{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "199b78e7",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "There are a lot of possibility for activation functions. The most common choice is the *Rectified Linear Unit or ReLU* (*i.e.* max(0, x)) that has the advantages of being fast to compute and fast to derive (the derivative is either 1 or 0, the value is either 0 or the input). There are tons of variations of it (ELU, LeakyReLU...) however I recommend to use them after you have a well working model already that uses ReLU: they help to marginally improve the accuracy, but they won't make the difference between 40% and 70% accuracy.\n",
    "\n",
    "**Warning**: ReLU is usually used *in-between* layers, but never at the end of the network: in fact, the network needs to output probabilities, while ReLU returns either 0 or x (that is whatever score). So at the end of the network you use the sigmoid (for binary classification problems) or softmax for multiclassification problems.\n",
    "\n",
    "<img src=\"assets/activation.png\" width=700px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf74d2df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T04:39:36.883987Z",
     "start_time": "2021-08-19T04:39:36.873524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relu: tensor([  0,   0,   0,   0,   0,   2,   5,  10, 100])\n",
      "Sigmoid: tensor([0.0000e+00, 4.5398e-05, 6.6929e-03, 1.1920e-01, 5.0000e-01, 8.8080e-01,\n",
      "        9.9331e-01, 9.9995e-01, 1.0000e+00])\n",
      "Tanh: tensor([-1.0000, -1.0000, -0.9999, -0.9640,  0.0000,  0.9640,  0.9999,  1.0000,\n",
      "         1.0000])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch \n",
    "\n",
    "test_input = torch.tensor([-100, -10, -5,-2, 0, 2, 5, 10, 100])\n",
    "relu_output = F.relu(test_input)\n",
    "sigmoid_output = F.sigmoid(test_input)\n",
    "tanh_output = torch.tanh(test_input)\n",
    "\n",
    "print(\"Relu:\", relu_output)\n",
    "print(\"Sigmoid:\", sigmoid_output)\n",
    "print(\"Tanh:\", tanh_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8e8e31",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bf2121",
   "metadata": {},
   "source": [
    "Using the correct loss function is crucial. For binary classification problems, a Binary Cross Entropy loss is often used (`nn.BCELoss()`), while for multiclassificatiown tasks like the MNIST one the `nn.CrossEntropyLoss()`. In case of regression problems, the most popular is the `nn.MSELoss`. \n",
    "\n",
    "In place of the `nn.CrossEntopyLoss()`, it is often used the Negative Log Likelihood Loss `nn.NLLLoss()`, that returns the log probabilities instead of the probabilities. \n",
    "However, it's very important to remember that:\n",
    "\n",
    "- You can use `nn.NLLLoss()` only if at the end of the Neural Network there is NO Softmax, but the `nn.LogSoftmax(x, dim=1)`\n",
    "\n",
    "- This means that if you want to get the probabilities for each class, you need to use the exponential function (that is the 'inverse' of the logarithm) by doing `torch.exp(output)` where output is what you get after applying the model to your input (`probabilities = torch.exp(model(X))`)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b693d1fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T09:58:34.411596Z",
     "start_time": "2021-08-18T09:58:34.401922Z"
    }
   },
   "source": [
    "## Optimizers\n",
    "\n",
    "Updating the weights is done by the optimizers. You can find a bunch of them here: https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "Each of them have some mathematical reasoning behind them. However, I'll leave it to your own curiosity. What you should know instead is:\n",
    "\n",
    "- The most used in the literature is the Adam optimizer\n",
    "\n",
    "`optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)`\n",
    "\n",
    "- A good alternative is often the Stochastic Gradient Descent\n",
    "\n",
    "`optimizer = optim.Adam(model.parameters(), lr=0.0001)`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478344cd",
   "metadata": {},
   "source": [
    "## Batch size\n",
    "\n",
    "Setting the right batch size is an hyperparameter that you will tune yourself. Often the limitations are imposed by the machine you are using, meaning you have to use a small batch size to make it to fit in your memory. Usually, power of two are used as batch size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5395cfa1",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Complete the previous notebooks, trying to understand each line of code. The text in between the cells should be descriptive enough for you to understand. If there are still pending things, we'll have a live coding session today. \n",
    "\n",
    "### Practice\n",
    "\n",
    "Intermediate: can you apply a Neural Network to tabular data? Take an old dataset and try to do it.\n",
    "Advanced: Can you use an MLP for your rock-paper-scissors problem? \n",
    "Next week you will be doing custom dataloaders, but I will attach the notebook already here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956bc8f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
