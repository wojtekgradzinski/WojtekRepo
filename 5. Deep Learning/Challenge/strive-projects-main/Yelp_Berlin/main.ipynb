{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "webpages = []\n",
    "for response in responses:\n",
    "    webpages.append(BeautifulSoup(response.content, 'html.parser'))\n",
    "\n",
    "len(webpages)  # Should be 24 for restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business urls\n",
    "# https://www.yelp.co.uk/search?cflt=homeservices&find_loc=Berlin%2C%20Germany\n",
    "# https://www.yelp.co.uk/search?cflt=restaurants&find_loc=Berlin%2C%20Germany\n",
    "\n",
    "businesses = [\"restaurants\", \"homeservices\"]\n",
    "\n",
    "main_url = f\"https://www.yelp.co.uk/search?cflt={businesses[0]}&find_loc=Berlin%2C%20Germany\"\n",
    "\n",
    "# other pages url (for restaurants case)\n",
    "# https://www.yelp.co.uk/search?cflt=restaurants&find_loc=Berlin%2C%20Germany&start=10\n",
    "# pages --> main_url + (start = range(10,231,10)), can automate finding ending index\n",
    "\n",
    "responses = [requests.get(main_url)]\n",
    "for index in range(10,231,10):\n",
    "    responses.append(requests.get(f\"{main_url}&start={index}\"))\n",
    "    delay = np.random.randint(1,6)\n",
    "    time.sleep(delay)\n",
    "\n",
    "# Class to get hrefs\n",
    "span_class = \"css-1pxmz4g\"\n",
    "\n",
    "# Needed regex here as strip was leaving some characters\n",
    "# Strangely those characters only appeared on appending but \n",
    "# not in a direct print\n",
    "\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "child_urls = []\n",
    "names = []\n",
    "for webpage in webpages:\n",
    "    items = webpage.findAll('span', class_ = span_class)\n",
    "    \n",
    "    for item in items:\n",
    "        names.append(regex.sub('', item.text))\n",
    "        child_urls.append(f\"https://www.yelp.co.uk{item.find('a')['href']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_page_class = \"css-e81eai\"\n",
    "response = requests.get(\"https://www.yelp.de/search?find_desc=restaurants&find_loc=Berlin&start=0\")\n",
    "soup = BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "num_pages = soup.findAll('span', class_ = num_page_class)\n",
    "\n",
    "print(num_pages[-3].text.strip('1 von'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({f\"{businesses[0]}_name\".title(): names, \"url\": child_urls})\n",
    "\n",
    "df.shape # Should be (240,2)\n",
    "df.to_csv('yelp_dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soupify all business pages\n",
    "df = pd.read_csv('yelp_dataset.csv')\n",
    "\n",
    "business_pages = []\n",
    "for session in tqdm(range(2)):\n",
    "    session_delay = np.random.randint(600,1200)\n",
    "\n",
    "    for url in tqdm(df['url']):\n",
    "        response = requests.get(url)\n",
    "        delay = np.random.randint(20,40)\n",
    "        time.sleep(delay)\n",
    "        business_pages.append(BeautifulSoup(response.content, 'html.parser'))\n",
    "    \n",
    "    time.sleep(session_delay)\n",
    "    \n",
    "print(len(business_pages)) # Should be 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_pages = []\n",
    "index = 0\n",
    "response = requests.get(df['url'][index])\n",
    "delay = np.random.randint(20,40)\n",
    "time.sleep(delay)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "business_pages.append(soup)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./webpages/business_page{index}.html\", \"w\") as file:\n",
    "    file.write(str(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of photos\n",
    "# photos_class = \"display--inline__373c0__1DbOG margin-l2__373c0__wvUpT border-color--default__373c0__2oFDT\"\n",
    "photos_class = \"css-ardur\"\n",
    "regex = re.compile('[^0-9]')\n",
    "\n",
    "df['photos_count'] = 0\n",
    "\n",
    "num_photos = business_pages[0].findAll('span', class_ = photos_class)[4]\n",
    "num_photos = int(regex.sub('', num_photos.text))\n",
    "\n",
    "df.loc[index, 'photos_count'] = num_photos\n",
    "\n",
    "df.to_csv('yelp_dataset.csv', mode = 'a', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stars\n",
    "# ratings_class =\"i-stars__373c0__1T6rz i-stars--large-4-half__373c0__2lYkD border-color--default__373c0__30oMI overflow--hidden__373c0__2B0kz\"\n",
    "stars_class = \"i-stars__373c0__1T6rz i-stars--large-4-half__373c0__2lYkD border-color--default__373c0__30oMI overflow--hidden__373c0__2B0kz\"\n",
    "regex = re.compile('[^0-9.]')\n",
    "\n",
    "df['stars'] = \"0\"\n",
    "\n",
    "stars = business_pages[0].findAll('div', class_ = stars_class)\n",
    "stars = stars[0]['aria-label']\n",
    "stars = regex.sub('', stars)\n",
    "\n",
    "df.loc[index, 'stars'] = stars\n",
    "df.to_csv('yelp_dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review count\n",
    "reviews_class =  \"css-bq71j2\"\n",
    "\n",
    "regex = re.compile('[^0-9]')\n",
    "\n",
    "df['review_count'] = 0\n",
    "\n",
    "review_count = business_pages[0].findAll('span', class_ = reviews_class)\n",
    "review_count = review_count[0].text\n",
    "review_count = int(regex.sub('', review_count))\n",
    "\n",
    "df.loc[index, 'review_count'] = review_count\n",
    "df.to_csv('yelp_dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claimed vs Unclaimed\n",
    "claim_class =\"css-166la90\"\n",
    "\n",
    "df['claimed'] = 0\n",
    "\n",
    "claimed = business_pages[0].findAll('a', class_ = claim_class)\n",
    "if claimed[0].text == \"Claimed\":\n",
    "    claimed = 1\n",
    "else: \n",
    "    claimed = 0\n",
    "\n",
    "df.loc[index, 'claimed'] = claimed\n",
    "df.to_csv('yelp_dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address\n",
    "address_class = \"css-e81eai\"\n",
    "\n",
    "df['address'] = \"0\"\n",
    "\n",
    "address = business_pages[0].findAll('p', class_ = address_class)\n",
    "address = address[1].text\n",
    "\n",
    "df.loc[index, 'address'] = address\n",
    "df.to_csv('yelp_dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hours\n",
    "day_class = \"day-of-the-week__373c0__124RF css-1h1j0y3\"\n",
    "time_class = \"no-wrap__373c0__2vNX7 css-1h1j0y3\"\n",
    "\n",
    "df['hours'] = \"0\"\n",
    "\n",
    "hours_table = business_pages[0].findAll('table')[0]\n",
    "\n",
    "rows = hours_table.findChildren('tr')\n",
    "\n",
    "times = []\n",
    "for idx, row in enumerate(rows):\n",
    "\n",
    "    times_text = row.findAll('p', class_ = time_class)\n",
    "    if times_text != []:\n",
    "        times.append(row.findAll('p', class_ = time_class)[0].text)\n",
    "\n",
    "days = ['Monday','Tuesday','Wednesday','Thursday','Friday']\n",
    "hours = {day:time for day in days for time in times}\n",
    "\n",
    "df.loc[index, 'hours'] = str(hours)\n",
    "df.to_csv('yelp_dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes\n",
    "# div parent class = \"arrange__373c0__UHqhV gutter-2__373c0__3Zpeq layout-wrap__373c0__34d4b layout-2-units__373c0__3CiAk border-color--default__373c0__2oFDT\"\n",
    "\n",
    "# \"margin-b3__373c0__q1DuY border-color--default__373c0__2oFDT\"\n",
    "\n",
    "outer_div = \"arrange__373c0__UHqhV gutter-2__373c0__3Zpeq layout-wrap__373c0__34d4b layout-2-units__373c0__3CiAk border-color--default__373c0__2oFDT\"\n",
    "# inner_div = \"arrange__373c0__UHqhV gutter-2__373c0__3Zpeq vertical-align-baseline__373c0__2s3Ze border-color--default__373c0__2oFDT\"\n",
    "inner_div = \"arrange-unit__373c0__1piwO border-color--default__373c0__2oFDT\"\n",
    "attributes = business_pages[0].find_all('div', class_ = outer_div)\n",
    "\n",
    "for attribute in attributes:\n",
    "    # print(attribute.find_all('span'))\n",
    "    print(attribute.text)\n",
    "\n",
    "    # for item in attribute.find_all('span'):\n",
    "    #     print()\n",
    "\n",
    "# inner_div2 = \"arrange__373c0__UHqhV gutter-2__373c0__3Zpeq vertical-align-baseline__373c0__2s3Ze border-color--default__373c0__2oFDT\"\n",
    "\n",
    "df['attributes'] = \"0\"\n",
    "\n",
    "# print(inner_div == inner_div2)\n",
    "\n",
    "\n",
    "# icon_greyed - 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = business_pages[0]\n",
    "\n",
    "def get_cpf():\n",
    "    driver = webdriver.Chrome(\"C:/Users/rajat/Downloads/chromedriver\")\n",
    "    driver.get(\"file:///C:/Users/rajat/OneDrive/Documents/GitHub/Strive/BuildWeeks/Week1/ymvc-berlin/webpages/business_page0.html\")\n",
    "    (driver.page_source).encode('utf-8')\n",
    "    # css_selector = \"button[class = ' css-174jypu']\"\n",
    "    # driver.find_element_by_css_selector(css_selector).click()\n",
    "    # time.sleep(np.random.randint(5,10))\n",
    "    # outer_div = \"arrange__373c0__UHqhV gutter-2__373c0__3Zpeq layout-wrap__373c0__34d4b layout-2-units__373c0__3CiAk border-color--default__373c0__2oFDT\"\n",
    "    # text = driver.find_elements_by_class_name(outer_div)\n",
    "    # print(text)\n",
    "\n",
    "get_cpf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "with open(f\"./webpages/business_page{index}.html\") as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_class = \"css-ardur\"\n",
    "stars_class = \"i-stars__373c0__1T6rz i-stars--large-4-half__373c0__2lYkD border-color--default__373c0__30oMI overflow--hidden__373c0__2B0kz\"\n",
    "reviews_class =  \"arrange-unit__373c0__1piwO arrange-unit-fill__373c0__17z0h border-color--default__373c0__2oFDT nowrap__373c0__1_N1j\"\n",
    "claim_class = \"border-color--default__373c0__2oFDT nowrap__373c0__1_N1j\"\n",
    "day_class = \"day-of-the-week__373c0__124RF css-1h1j0y3\"\n",
    "time_class = \"no-wrap__373c0__2vNX7 css-1h1j0y3\"\n",
    "stars_class = \"display--inline__373c0__2SfH_ border-color--default__373c0__30oMI\"\n",
    "categories_class = \"css-bq71j2\" \n",
    "attributes_outer_div = \"arrange__373c0__UHqhV gutter-2__373c0__3Zpeq layout-wrap__373c0__34d4b layout-2-units__373c0__3CiAk border-color--default__373c0__2oFDT\"\n",
    "\n",
    "\n",
    "days = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday', 'Sunday']\n",
    "\n",
    "total_webpages = range(68,241)\n",
    "n = 8\n",
    "sessions = [total_webpages[i:i+n] for i in range(0, len(total_webpages), n)]\n",
    "\n",
    "# print(sessions)\n",
    "for session, session_range in tqdm(enumerate(sessions)):\n",
    "    # print(session)\n",
    "    for index in tqdm(session_range):\n",
    "\n",
    "        # # with open(f\"./webpages/business_page{index}.html\", encoding = \"utf8\") as f:\n",
    "        # #     soup = BeautifulSoup(f, 'html.parser')\n",
    "        df['url'][index]\n",
    "        response = requests.get(df['url'][index])\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Numbre of photos\n",
    "        try:\n",
    "            regex = re.compile('[^0-9]')\n",
    "\n",
    "            num_photos = soup.findAll('span', class_ = photos_class)[4]\n",
    "            num_photos = int(regex.sub('', num_photos.text))\n",
    "\n",
    "            df.loc[index, 'photos_count'] = num_photos\n",
    "            \n",
    "        \n",
    "        except (ValueError, IndexError):\n",
    "            df.loc[index, 'photos_count'] = None\n",
    "\n",
    "        # Stars\n",
    "        # ratings_class =\"i-stars__373c0__1T6rz i-stars--large-4-half__373c0__2lYkD border-color--default__373c0__30oMI overflow--hidden__373c0__2B0kz\" \n",
    "        # stars_class = \"i-stars__373c0__1T6rz i-stars--large-4-half__373c0__2lYkD border-color--default__373c0__30oMI overflow--hidden__373c0__2B0kz\"\n",
    "        regex = re.compile('[^0-9.]')\n",
    "\n",
    "        stars = soup.findAll('span', class_ = stars_class)[0]\n",
    "        stars = stars.findChildren('div')\n",
    "        stars = stars[0]['aria-label']\n",
    "        stars = regex.sub('', stars)\n",
    "\n",
    "        df.loc[index, 'stars'] = stars\n",
    "\n",
    "        # Review count\n",
    "\n",
    "        regex = re.compile('[^0-9]')\n",
    "\n",
    "        review_count = soup.findAll('div', class_ = reviews_class)\n",
    "        review_count = review_count[0].text\n",
    "        review_count = int(regex.sub('', review_count))\n",
    "\n",
    "        df.loc[index, 'review_count'] = review_count\n",
    "\n",
    "        # Claimed vs Unclaimed\n",
    "\n",
    "        claimed = soup.findAll('div', class_ = claim_class)\n",
    "\n",
    "        if claimed[0].text.strip() == \"Claimed\":\n",
    "            claimed = 1\n",
    "        else: \n",
    "            claimed = 0\n",
    "\n",
    "        df.loc[index, 'claimed'] = claimed\n",
    "\n",
    "        # Address\n",
    "\n",
    "        address = soup.findAll('address', class_ = \"\")\n",
    "\n",
    "        location = \"\"\n",
    "        for element in address:\n",
    "\n",
    "            for p in element.findAll('p')[:-1]:\n",
    "                location += p.text\n",
    "\n",
    "        df.loc[index, 'address'] = location\n",
    "\n",
    "        # Hours\n",
    "\n",
    "        hours_table = soup.findAll('table')[0]\n",
    "\n",
    "        rows = hours_table.findChildren('tr')\n",
    "\n",
    "        times = []\n",
    "        for idx, row in enumerate(rows):\n",
    "\n",
    "            times_text = row.findAll('p', class_ = time_class)\n",
    "            if times_text != []:\n",
    "                times.append(row.findAll('p', class_ = time_class)[0].text)\n",
    "\n",
    "        \n",
    "        hours = {day:time for day in days for time in times}\n",
    "\n",
    "        df.loc[index, 'hours'] = str(hours)\n",
    "\n",
    "        # Categories\n",
    "        categories_elements = soup.findAll('span', class_ = categories_class)\n",
    "\n",
    "        categories = []\n",
    "        for element in categories_elements:\n",
    "            category.findAll('a')\n",
    "\n",
    "            for category in element.findAll('a'):\n",
    "                categories.append(category.text)\n",
    "\n",
    "        categories = \",\".join(categories)\n",
    "\n",
    "        df.loc[index,'categories'] = categories\n",
    "\n",
    "        # Attributes\n",
    "\n",
    "        attributes_elements = soup.findAll('div', class_ = attributes_outer_div)\n",
    "\n",
    "        attributes = []\n",
    "        for div_element in attributes_elements:\n",
    "\n",
    "            for span_element in div_element.findAll('span')[1::2]:\n",
    "                attributes.append(span_element.text)\n",
    "        attributes = \",\".join(attributes)\n",
    "\n",
    "        df.loc[index,'attributes'] = attributes\n",
    "        \n",
    "        # Saving to dataset\n",
    "        df.to_csv('yelp_dataset.csv', index = False)\n",
    "        delay = np.random.randint(40,100)\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    time.sleep(np.random.randint(150,1200))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./webpages/business_page{index}.html\", encoding = \"utf8\") as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')\n",
    "    # Stars\n",
    "    # ratings_class =\"i-stars__373c0__1T6rz i-stars--large-4-half__373c0__2lYkD border-color--default__373c0__30oMI overflow--hidden__373c0__2B0kz\"\n",
    "stars_class = \"display--inline__373c0__2SfH_ border-color--default__373c0__30oMI\"\n",
    "# stars_class = \"i-stars__373c0__1T6rz i-stars--large-4-half__373c0__2lYkD border-color--default__373c0__30oMI overflow--hidden__373c0__2B0kz\"\n",
    "regex = re.compile('[^0-9.]')\n",
    "\n",
    "stars = soup.findAll('span', class_ = stars_class)[0]\n",
    "stars = stars.findChildren('div')\n",
    "stars = stars[0]['aria-label']\n",
    "stars = regex.sub('', stars)\n",
    "stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"i-stars__373c0__1T6rz i-stars--large-5__373c0__1GcGD border-color--default__373c0__30oMI overflow--hidden__373c0__2B0kz\" == \"i-stars__373c0__1T6rz i-stars--large-4-half__373c0__2lYkD border-color--default__373c0__30oMI overflow--hidden__373c0__2B0kz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review count\n",
    "reviews_class =  \"arrange-unit__373c0__1piwO arrange-unit-fill__373c0__17z0h border-color--default__373c0__2oFDT nowrap__373c0__1_N1j\"\n",
    "\n",
    "regex = re.compile('[^0-9]')\n",
    "\n",
    "review_count = soup.findAll('div', class_ = reviews_class)\n",
    "review_count[0].text\n",
    "# review_count = review_count[0].text\n",
    "# review_count = int(regex.sub('', review_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(df['url'][1])\n",
    "\n",
    "claim_class = \"border-color--default__373c0__2oFDT nowrap__373c0__1_N1j\"\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "claimed = soup.findAll('div', class_ = claim_class)\n",
    "\n",
    "# if claimed[0].text.strip() == \"Claimed\":\n",
    "#     claimed = 1\n",
    "# else: \n",
    "#     claimed = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claimed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_class = \"arrange-unit__373c0__1piwO arrange-unit-fill__373c0__17z0h border-color--default__373c0__2oFDT\"\n",
    "\n",
    "response = requests.get(df['url'][1])\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "address = soup.findAll('address', class_ = \"\")\n",
    "\n",
    "location = \"\"\n",
    "for element in address:\n",
    "\n",
    "    for p in element.findAll('p')[:-1]:\n",
    "        location += p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories\n",
    "categories_class = \"css-bq71j2\" \n",
    "\n",
    "response = requests.get(df['url'][2])\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "categories = soup.findAll('span', class_ = categories_class)\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories\n",
    "categories_class = \"css-bq71j2\" \n",
    "\n",
    "response = requests.get(df['url'][2])\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "categories_elements = soup.findAll('span', class_ = categories_class)\n",
    "\n",
    "categories = []\n",
    "# for category in categories_element:\n",
    "#     categories += category.text\n",
    "for element in categories_elements:\n",
    "    category.findAll('a')\n",
    "\n",
    "    for category in element.findAll('a'):\n",
    "        categories.append(category.text)\n",
    "\n",
    "categories = \",\".join(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_outer_div = \"arrange__373c0__UHqhV gutter-2__373c0__3Zpeq layout-wrap__373c0__34d4b layout-2-units__373c0__3CiAk border-color--default__373c0__2oFDT\"\n",
    "# inner_div = \"arrange__373c0__UHqhV gutter-2__373c0__3Zpeq vertical-align-baseline__373c0__2s3Ze border-color--default__373c0__2oFDT\"\n",
    "inner_div = \"arrange-unit__373c0__1piwO border-color--default__373c0__2oFDT\"\n",
    "\n",
    "response = requests.get(df['url'][2])\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "attributes_elements = soup.findAll('div', class_ = attributes_outer_div)\n",
    "\n",
    "attributes = []\n",
    "for div_element in attributes_elements:\n",
    "\n",
    "    for span_element in div_element.findAll('span')[1::2]:\n",
    "        attributes.append(span_element.text)\n",
    "attributes = \",\".join(attributes)\n",
    "attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    business = 'japanese'\n",
    "    num_pages = 2\n",
    "\n",
    "    main_url = f\"https://www.yelp.co.uk/search?cflt={business}&find_loc=Berlin%2C%20Germany\"\n",
    "    responses = [requests.get(main_url)]\n",
    "\n",
    "    for index in tqdm(range(10, num_pages*10, 10), desc=f\"Requesting pages for {business}\"):\n",
    "        responses.append(requests.get(f\"{main_url}&start={index}\"))\n",
    "        delay = np.random.randint(1,6)\n",
    "        time.sleep(delay)\n",
    "\n",
    "    webpages = []\n",
    "    for response in responses:\n",
    "        webpages.append(BeautifulSoup(response.content, 'html.parser'))\n",
    "\n",
    "    # Class to get hrefs\n",
    "    span_class = \"css-1pxmz4g\"\n",
    "\n",
    "    # Needed regex here as strip was leaving some characters Strangely those\n",
    "    # characters only appeared on appending but not in a direct print\n",
    "\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    child_urls = []\n",
    "    names = []\n",
    "    for webpage in webpages:\n",
    "        items = webpage.findAll('span', class_ = span_class)\n",
    "        print(items)\n",
    "\n",
    "        for item in items:\n",
    "            names.append(regex.sub('', item.text))\n",
    "            if item.find('a') != None:\n",
    "                print(item.find('a'))\n",
    "            # print(item.findAll('a', class_ = \"css-166la90\"))\n",
    "            # child_urls.append(f\"https://www.yelp.co.uk{item.find('a')['href']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Closed', 'Closed', '7:00 PM - 12:00 AM (Next day)', '7:00 PM - 12:00 AM (Next day)', '7:00 PM - 12:00 AM (Next day)', '7:00 PM - 12:00 AM (Next day)', 'Closed']\nClosed\nClosed\n7:00 PM - 12:00 AM (Next day)\n7:00 PM - 12:00 AM (Next day)\n7:00 PM - 12:00 AM (Next day)\n7:00 PM - 12:00 AM (Next day)\nClosed\n{'Monday': 'Closed', 'Tuesday': 'Closed', 'Wednesday': '7:00 PM - 12:00 AM (Next day)', 'Thursday': '7:00 PM - 12:00 AM (Next day)', 'Friday': '7:00 PM - 12:00 AM (Next day)', 'Saturday': '7:00 PM - 12:00 AM (Next day)', 'Sunday': 'Closed'}\n"
     ]
    }
   ],
   "source": [
    "day_class = \"day-of-the-week__373c0__124RF css-1h1j0y3\"\n",
    "time_class = \"no-wrap__373c0__2vNX7 css-1h1j0y3\"\n",
    "\n",
    "days = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "response = requests.get(\"https://www.yelp.co.uk/biz/lorenz-adlon-esszimmer-berlin\")\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "hours_table = soup.findAll('table')[0]\n",
    "\n",
    "rows = hours_table.findChildren('tr')\n",
    "\n",
    "times = []\n",
    "for idx, row in enumerate(rows):\n",
    "\n",
    "    times_text = row.findAll('p', class_ = time_class)\n",
    "    if times_text != []:\n",
    "        times.append(row.findAll('p', class_ = time_class)[0].text)\n",
    "\n",
    "print(times)\n",
    "hours = {day:time for day,time in zip(days,times)}\n",
    "for time in times:\n",
    "    print(time)\n",
    "print(hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "execution_count": 470
    }
   ],
   "source": [
    "# Euros\n",
    "euros_class = \"css-1xxismk\"\n",
    "\n",
    "euros_category = soup.findAll('span', class_ = euros_class)\n",
    "euros_category = len(euros_category[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "68274f4789de8facababa72c5292f6e9ee0042994feee1bf982b457f4d00f7e0"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('strivenev': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}