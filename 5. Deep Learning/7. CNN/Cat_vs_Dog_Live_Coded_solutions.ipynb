{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "    <h2 align=\"center\"> Cats vs Dogs </h2>\n",
    "    <h2 align=\"center\" style=\"color:#01ff84\"> Loading your dataset</h2>\n",
    "<div>\n",
    "\n",
    "<br>\n",
    "    \n",
    "So far we've been working with fairly artificial datasets that you wouldn't typically be using in real projects. Instead, you'll likely be dealing with full-sized images like you'd get from smart phone cameras. In this notebook, we'll look at how to load images and use them to train neural networks.\n",
    "\n",
    "We'll be using a [dataset of cat and dog photos](https://www.kaggle.com/c/dogs-vs-cats) available from Kaggle. Here are a couple example images:\n",
    "\n",
    "<img src=\"imgs/dog.png\" width=\"200\" height=\"40\" />\n",
    "<img src=\"imgs/cat.png\" width=\"200\" height=\"40\" />\n",
    "    \n",
    "\n",
    "We'll use this dataset to train a neural network that can differentiate between cats and dogs. These days it doesn't seem like a big accomplishment, but five years ago it was a serious challenge for computer vision systems.\n",
    "    \n",
    "<br>\n",
    "    \n",
    "Start importing the needed libraries:\n",
    "    \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%matplotlib inline\r\n",
    "%config InlineBackend.figure_format = 'retina'\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "from torchvision import datasets, transforms\r\n"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T14:40:28.509241Z",
     "start_time": "2021-08-24T14:40:28.433306Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The easiest way to load image data is with `datasets.ImageFolder` from `torchvision` ([documentation](http://pytorch.org/docs/master/torchvision/datasets.html#imagefolder)). In general you'll use `ImageFolder` like so:\n",
    "\n",
    "```python\n",
    "dataset = datasets.ImageFolder('path/to/data', transform=transform)\n",
    "```\n",
    "\n",
    "where `'path/to/data'` is the file path to the data directory and `transform` is a list of processing steps built with the [`transforms`](http://pytorch.org/docs/master/torchvision/transforms.html) module from `torchvision`. ImageFolder expects the files and directories to be constructed like so:\n",
    "```\n",
    "root/dog/xxx.png\n",
    "root/dog/xxy.png\n",
    "root/dog/xxz.png\n",
    "\n",
    "root/cat/123.png\n",
    "root/cat/nsdf3.png\n",
    "root/cat/asd932_.png\n",
    "```\n",
    "\n",
    "where each class has it's own directory (`cat` and `dog`) for the images. The images are then labeled with the class taken from the directory name. So here, the image `123.png` would be loaded with the class label `cat`. You can download the dataset already structured like this [from here](https://s3.amazonaws.com/content.udacity-data.com/nd089/Cat_Dog_data.zip). They are already splitted into a training set and test set.\n",
    "\n",
    " \n",
    ">**Exercise:** Download the dataset and place the train and test set in the `datasets/cat_vs_dog` folder. If you're cloning this from github, you should have it in `../datasets/`. So first create the `cat_vs_dog` folder in `datasets` and verify that the data are there by running `ls ../datasets/cat_vs_dog` (or your custom path if you changed it)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data = datasets.ImageFolder(\"dataset\\dogs-vs-cats\\train\", transform=)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Great! Now that you have downloaded your data, you need to define the transformations to be passed to the `ImageFolder` function. You have already used them with the MNIST dataset (see the Data Augmentation workbook in Pytorch). While for MNIST you were passing the transformation in the following line of code\n",
    "\n",
    "`datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)`\n",
    "\n",
    "here you do it in the `ImageFolder` method. You can think it as a way to work on every dataset, not only on the MNIST one.\n",
    "\n",
    "\n",
    "### Transforms\n",
    "\n",
    "When you load in the data with `ImageFolder`, you'll need to define some transforms. For example, the images are different sizes but we'll need them to all be the same size for training. You can either resize them with `transforms.Resize()` or crop with `transforms.CenterCrop()`, `transforms.RandomResizedCrop()`, etc. We'll also need to convert the images to PyTorch tensors with `transforms.ToTensor()`. Typically you'll combine these transforms into a pipeline with `transforms.Compose()`, which accepts a list of transforms and runs them in sequence. \n",
    "\n",
    "As in the other notebook, you can use the following transformations:\n",
    "\n",
    "```python\n",
    "transform = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.5, 0.5, 0.5], \n",
    "                                                            [0.5, 0.5, 0.5])])\n",
    "\n",
    "```\n",
    "\n",
    "**WARNING!** Remember that transformation are super useful for \"augmenting\" your training data, so that you make your network less vulnerable to different sizes, rotations, or cropping. However, when you are on the test data, there is no need of augmenting the data! Actually, it is not a good practice to do that because there would be very repetitive test data that invalidates your score.\n",
    "\n",
    "For this reason, define two different transformations for training and test data (remember that `ToTensor()` and normalization are necessary also for the test data, as well as the resizing (you can use `transforms.Resize(size)` for it):\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "data_dir = 'datasets/cat_vs_dog'\r\n",
    "\r\n",
    "# TODO: Define transforms for the training data and testing data\r\n",
    "train_transforms = transforms.Compose([transforms.Resize(255),\r\n",
    "                                       transforms.RandomRotation(30),\r\n",
    "                                       transforms.RandomResizedCrop(224),\r\n",
    "                                       transforms.RandomHorizontalFlip(),\r\n",
    "                                       transforms.ToTensor(),\r\n",
    "                                       transforms.Normalize([0.5, 0.5, 0.5],\r\n",
    "                                                            [0.5, 0.5, 0.5])])\r\n",
    "\r\n",
    "test_transforms = transforms.Compose([transforms.Resize(255),\r\n",
    "                                      transforms.CenterCrop(224),\r\n",
    "                                      transforms.ToTensor(),\r\n",
    "                                      transforms.Normalize([0.5, 0.5, 0.5],\r\n",
    "                                                           [0.5, 0.5, 0.5])])\r\n",
    "\r\n",
    "# Pass transforms in here, then run the next cell to see how the transforms look\r\n",
    "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\r\n",
    "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\r\n"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T15:00:30.867321Z",
     "start_time": "2021-08-24T15:00:30.490648Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that you have you have defined the needed transformation, it's time to build the Data loader itself!\n",
    "\n",
    "### Data Loaders\n",
    "\n",
    "With the `ImageFolder` loaded, you have to pass it to a [`DataLoader`](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader). The `DataLoader` takes a dataset (such as you would get from `ImageFolder`) and returns batches of images and the corresponding labels. You can set various parameters like the batch size and if the data is shuffled after each epoch.\n",
    "\n",
    "```python\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "```\n",
    "\n",
    "Here `dataloader` is a [generator](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/). To get data out of it, you need to loop through it or convert it to an iterator and call `next()`.\n",
    "\n",
    "```python\n",
    "# Looping through it, get a batch on each loop \n",
    "for images, labels in dataloader:\n",
    "    pass\n",
    "\n",
    "# Get one batch\n",
    "images, labels = next(iter(dataloader))\n",
    "```\n",
    " \n",
    ">**Exercise:** Build the dataloader for both the train and test data. Choose the batch size that fits your memory. \n",
    "**Remember** NOT TO shuffle the test data! "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True)\r\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=4)\r\n",
    "trainloader"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T15:05:54.429044Z",
     "start_time": "2021-08-24T15:05:54.412652Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def imshow(image, ax=None, title=None, normalize=True):\r\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\r\n",
    "    if ax is None:\r\n",
    "        fig, ax = plt.subplots()\r\n",
    "    image = image.numpy().transpose((1, 2, 0))\r\n",
    "\r\n",
    "    if normalize:\r\n",
    "        mean = np.array([0.485, 0.456, 0.406])\r\n",
    "        std = np.array([0.229, 0.224, 0.225])\r\n",
    "        image = std * image + mean\r\n",
    "        image = np.clip(image, 0, 1)\r\n",
    "\r\n",
    "    ax.imshow(image)\r\n",
    "    ax.spines['top'].set_visible(False)\r\n",
    "    ax.spines['right'].set_visible(False)\r\n",
    "    ax.spines['left'].set_visible(False)\r\n",
    "    ax.spines['bottom'].set_visible(False)\r\n",
    "    ax.tick_params(axis='both', length=0)\r\n",
    "    ax.set_xticklabels('')\r\n",
    "    ax.set_yticklabels('')\r\n",
    "\r\n",
    "    return ax\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T15:09:00.689555Z",
     "start_time": "2021-08-24T15:09:00.481820Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# Run this to test your data loaders\r\n",
    "images, labels = next(iter(trainloader))\r\n",
    "imshow(images[0], normalize=True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, now let's create a simple Convolutional Neural Network for this task!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**N.B.** When building a convolutional neural network, be careful for making all the shapes to match (with Pytorch, in Keras is handled automatically).\n",
    "\n",
    "\n",
    "### Unfolding the numbers meaning\n",
    "\n",
    " `self.conv1 = nn.Conv2d(3, 32, 5) `\n",
    " \n",
    " What are the 3, 32 and 5?\n",
    " \n",
    " Let make it more general:\n",
    " \n",
    " `self.conv1 = nn.Conv2d(in_channels=param1, out_channels=param2, kernel_size=param3,  stride=param4) `\n",
    " \n",
    " "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#input formula\r\n",
    "W = 1 +(N + 2*P - K)/S"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def  output_shape(input_size, padding, kernel_size, stride):\r\n",
    "    formula = 1 + ( input_size + 2* padding - kernel_size) / stride\r\n",
    "    return formula "
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T15:19:27.850303Z",
     "start_time": "2021-08-24T15:19:27.839098Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "output_shape(input_size=224, padding=0, kernel_size=5, stride=1)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T15:20:13.716500Z",
     "start_time": "2021-08-24T15:20:13.709759Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#calculating conv1 output  multiply by numbers of filters\r\n",
    "220*220*32\r\n",
    "#apply pooling ( pooling is 2 so everything is devided by 2)\r\n",
    "110*110*32\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "output_shape(input_size=110, kernel_size=5, padding=0, stride=1)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T15:22:03.860259Z",
     "start_time": "2021-08-24T15:22:03.852886Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#calculating conv2 output  multiply by numbers of filters\r\n",
    "106*106*16\r\n",
    "#apply pooling ( pooling is 2 so everything is devided by 2)\r\n",
    "53*53*16"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "\r\n",
    "class Net(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(Net, self).__init__()\r\n",
    "        self.conv1 = nn.Conv2d(5, 32, 5)\r\n",
    "        self.pool = nn.MaxPool2d(2, 2)\r\n",
    "        self.conv2 = nn.Conv2d(32, 16, 5)\r\n",
    "        self.fc1 = nn.Linear(16*53*53, 128)\r\n",
    "        self.fc2 = nn.Linear(128, 64)\r\n",
    "        self.fc3 = nn.Linear(64, 2) #since we use softmax we need to output 1cat or 0dog. If we were using Sigmoid the output is one\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.pool(F.relu(self.conv1(x)))\r\n",
    "        x = self.pool(F.relu(self.conv2(x)))\r\n",
    "        x = x.view(x.shape[0], -1)\r\n",
    "        x = F.relu(self.fc1(x))\r\n",
    "        x = F.relu(self.fc2(x))\r\n",
    "        x = self.fc3(x)\r\n",
    "        x = F.log_softmax(x, dim=1)\r\n",
    "        return x\r\n",
    "\r\n",
    "\r\n",
    "net = Net()\r\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\r\n",
    "criterion = nn.NLLLoss()\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T15:29:02.674000Z",
     "start_time": "2021-08-24T15:29:02.580751Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Why do we reshape in the middle of the forward pass?\n",
    "\n",
    "As you remember from the class, the output of a convolutional layer is always a 3D volume! For this reason, since the output channels of the conv2 layer is 16 and the feature maps have size 5x5, then the input of the fc1 layer must be reshaped to have shape `(batch_size, 16 * 5 * 5)`.\n",
    "\n",
    "Now the question is: what will be the difference in the training of this network with respect to the fully-connected one you are used to?\n",
    "\n",
    "None, except for the fact that you do not reshape the input to be a vector, but you keep the shape as a volume! \n",
    "\n",
    ">**Exercise:** Implement a Convolutional Neural Network for the cat vs dog challenge, such that:\n",
    "> - The input images have shape 28x28 and three RGB channels\n",
    "> - You have 2 Conv2d layer with MaxPool2D in the middle and two fully-connected layer at the end.\n",
    "> - You can decide yourself the rest of the hyperparameters (kernel size, number of filters...)\n",
    "> - Train and evaluate your model\n",
    "\n",
    "Following there's an helper function to visualize your prediction once your model has been built.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "class_list = train_data.classes\r\n",
    "\r\n",
    "def view_classify_general(img, ps, class_list):\r\n",
    "    ''' Function for viewing an image and it's predicted classes.\r\n",
    "    '''\r\n",
    "    ps = ps.data.numpy().squeeze()\r\n",
    "\r\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\r\n",
    "    imshow(img, ax=ax1, normalize=True)\r\n",
    "    ax1.axis('off')\r\n",
    "    ax2.barh(np.arange(len(class_list)), ps)\r\n",
    "    ax2.set_aspect(0.1)\r\n",
    "    ax2.set_yticks(np.arange(len(class_list)))\r\n",
    "    ax2.set_yticklabels([x for x in class_list], size='small');\r\n",
    "    ax2.set_title('Class Probability')\r\n",
    "    ax2.set_xlim(0, 1.1)\r\n",
    "\r\n",
    "    plt.tight_layout()\r\n",
    "\r\n",
    "images, labels = next(iter(trainloader))\r\n",
    "img, label = images[0], labels[0]\r\n",
    "\r\n",
    "# Forward pass, get our logits\r\n",
    "logits = net(img.view(1, *images[0].shape))\r\n",
    "# Calculate the loss with the logits and the labels\r\n",
    "ps = torch.exp(logits)\r\n",
    "    \r\n",
    "view_classify_general(img, ps, class_list)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T15:34:33.300558Z",
     "start_time": "2021-08-24T15:34:32.901042Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img, label = images[0], labels[0]"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T15:31:17.689773Z",
     "start_time": "2021-08-24T15:31:17.686416Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "img.view(1, *img.shape)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T15:33:13.299760Z",
     "start_time": "2021-08-24T15:33:13.287974Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "logits = net(img.view(1, *images[0].shape))"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T15:33:36.460935Z",
     "start_time": "2021-08-24T15:33:36.239680Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.exp(logits)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T15:34:19.267100Z",
     "start_time": "2021-08-24T15:34:19.259819Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You should get something cute like this:\n",
    "\n",
    "![image](imgs/cat_pred.png)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_dir = 'datasets/cat_vs_dog'\r\n",
    "\r\n",
    "# TODO: Define transforms for the training data and testing data\r\n",
    "train_transforms = transforms.Compose([transforms.Resize(40),\r\n",
    "                                       transforms.RandomRotation(30),\r\n",
    "                                       transforms.RandomResizedCrop(28),\r\n",
    "                                       transforms.RandomHorizontalFlip(),\r\n",
    "                                       transforms.ToTensor(),\r\n",
    "                                       transforms.Normalize([0.5, 0.5, 0.5],\r\n",
    "                                                            [0.5, 0.5, 0.5])])\r\n",
    "\r\n",
    "test_transforms = transforms.Compose([transforms.Resize(28),\r\n",
    "                                      transforms.CenterCrop(28),\r\n",
    "                                      transforms.ToTensor(),\r\n",
    "                                      transforms.Normalize([0.5, 0.5, 0.5],\r\n",
    "                                                           [0.5, 0.5, 0.5])])\r\n",
    "\r\n",
    "# Pass transforms in here, then run the next cell to see how the transforms look\r\n",
    "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\r\n",
    "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\r\n",
    "\r\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=5, shuffle=True)\r\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=5)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T15:46:27.712685Z",
     "start_time": "2021-08-24T15:46:27.540744Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Output of the first convolutional layer:\n",
    "    \n",
    "W2 = (W1 - F + 2P)/S + 1\n",
    "\n",
    "- W1 = 28\n",
    "- F = 5\n",
    "- P = 0\n",
    "- S = 1\n",
    "\n",
    "--> W2 = 24\n",
    "\n",
    "So a tensor of dimensions batch_size * 24 * 24 * 64\n",
    "\n",
    "where 32 is the number of filters in the conv1 layer.\n",
    "\n",
    "Output of the first Max Pooling:\n",
    "    \n",
    "batch_size * 12 * 12 * 64\n",
    "\n",
    "Output of the second convolutional layer:\n",
    "    \n",
    "W2 = (W1 - F + 2P)/S + 1\n",
    "\n",
    "with:\n",
    "    \n",
    "- W1 = 12\n",
    "- F = 5\n",
    "- P = 0\n",
    "- S = 1\n",
    "\n",
    "--> W2 = (12 - 5 + 0)/1 + 1 = 8\n",
    "\n",
    "So a tensor of dimensions batch_size * 8 * 8 * 64\n",
    "\n",
    "where 64 is the number of filters in the conv2 layer.\n",
    "\n",
    "Output of the second Max Pooling:\n",
    "    \n",
    "a tensor of dimensions batch_size * 4 * 4 * 64.\n",
    "\n",
    "Why do we do these considerations? Because the Linear (Fully-connected layers) need to know the size of the input is receiving."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "\r\n",
    "class SmallNet(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(SmallNet, self).__init__()\r\n",
    "        self.conv1 = nn.Conv2d(3, 64, 5)\r\n",
    "        self.bn = nn.BatchNorm2d(64)\r\n",
    "        self.pool = nn.MaxPool2d(2, 2)\r\n",
    "        self.conv2 = nn.Conv2d(64, 64, 5)\r\n",
    "        self.fc1 = nn.Linear(4*4*64, 128)\r\n",
    "        self.fc2 = nn.Linear(128, 2)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.pool(F.relu(self.bn(self.conv1(x))))\r\n",
    "        x = self.pool(F.relu(self.conv2(x)))\r\n",
    "        x = x.view(x.shape[0], -1)\r\n",
    "        x = F.relu(self.fc1(x))\r\n",
    "        x = F.dropout(x)\r\n",
    "        x = self.fc2(x)\r\n",
    "        #x = F.dropout(x)\r\n",
    "        x = F.log_softmax(x, dim=1)\r\n",
    "        return x\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T15:46:28.968115Z",
     "start_time": "2021-08-24T15:46:28.954561Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(train_data.classes)\r\n",
    "print(test_data.classes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = SmallNet()\r\n",
    "\r\n",
    "ps = torch.exp(model(image))"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T15:41:13.976950Z",
     "start_time": "2021-08-24T15:41:13.971230Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "top_prob, top_class = ps.topk(1, dim=1)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T15:41:55.178156Z",
     "start_time": "2021-08-24T15:41:55.173998Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "top_class"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T15:42:42.832194Z",
     "start_time": "2021-08-24T15:42:42.825406Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "top_class.shape\r\n",
    "\r\n",
    "print(train_data.class_to_idx)\r\n",
    "print(test_data.class_to_idx)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T15:43:46.978188Z",
     "start_time": "2021-08-24T15:43:46.973063Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "top_class == label.view(*top_class.shape)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T15:44:01.608784Z",
     "start_time": "2021-08-24T15:44:01.602621Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "[True, False, True, False, False]\r\n",
    "-> [1.0, 0.0, 1.0, 0.0, 0.0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch import optim\r\n",
    "model = SmallNet()\r\n",
    "criterion = nn.NLLLoss()\r\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\r\n",
    "\r\n",
    "epochs = 10\r\n",
    "steps = 0\r\n",
    "\r\n",
    "train_losses, test_losses = [], []\r\n",
    "for e in range(epochs):\r\n",
    "    running_loss = 0\r\n",
    "    for images, labels in trainloader:\r\n",
    "        \r\n",
    "        optimizer.zero_grad()\r\n",
    "        \r\n",
    "        log_ps = model(images)\r\n",
    "        loss = criterion(log_ps, labels)\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        \r\n",
    "        running_loss += loss.item()\r\n",
    "        \r\n",
    "    else:\r\n",
    "        test_loss = 0\r\n",
    "        accuracy = 0\r\n",
    "        \r\n",
    "        # Turn off gradients for validation, saves memory and computations\r\n",
    "        with torch.no_grad():\r\n",
    "            model.eval()\r\n",
    "            for images, labels in testloader:\r\n",
    "                log_ps = model(images)\r\n",
    "                test_loss += criterion(log_ps, labels)\r\n",
    "                \r\n",
    "                ps = torch.exp(log_ps)\r\n",
    "                top_p, top_class = ps.topk(1, dim=1)\r\n",
    "                equals = top_class == labels.view(*top_class.shape)\r\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\r\n",
    "        \r\n",
    "        model.train()\r\n",
    "        \r\n",
    "        train_losses.append(running_loss/len(trainloader))\r\n",
    "        test_losses.append(test_loss/len(testloader))\r\n",
    "\r\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\r\n",
    "              \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\r\n",
    "              \"Test Loss: {:.3f}.. \".format(test_losses[-1]),\r\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T15:51:49.116055Z",
     "start_time": "2021-08-24T15:46:35.252467Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "images, labels = next(iter(trainloader))\r\n",
    "img, label = images[0], labels[0]\r\n",
    "# Flatten images\r\n",
    "# Forward pass, get our logits\r\n",
    "logits = model(img.view(1, *images[0].shape))\r\n",
    "# Calculate the loss with the logits and the labels\r\n",
    "ps = torch.exp(logits)\r\n",
    "    \r\n",
    "view_classify_general(img, ps, class_list)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T15:52:03.891378Z",
     "start_time": "2021-08-24T15:52:03.635135Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('deep': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "interpreter": {
   "hash": "06eb6164245d5dc111fc587aaaaee211f1064893786d78224bfe45d400023d11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}