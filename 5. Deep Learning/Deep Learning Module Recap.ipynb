{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "conceptual-vacuum",
   "metadata": {},
   "source": [
    "## What is Deep Learning?\n",
    "\n",
    "A field of Machine Learning that is a field of AI. It's called Deep Learning because it exploits architectures with multiple \"hidden layers\", called Neural Networks, giving a certain \"depth\" to the model.\n",
    "\n",
    "![image](https://t44dz3y7fq02vi4w64ej6i5t-wpengine.netdna-ssl.com/wp-content/uploads/2018/06/deep-learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-sheffield",
   "metadata": {},
   "source": [
    "## What are common components of a Neural Network?\n",
    "\n",
    "**Activation functions**\n",
    "\n",
    "Examples of activation functions:\n",
    "\n",
    "*Sigmoid*: `1/(1+e^(-x))`, output between 0 and 1. Often used for transforming scores into probabilities.\n",
    "\n",
    "![image](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-neighborhood",
   "metadata": {},
   "source": [
    "*ReLU*: max(0, x), output: between 0 and +inf, used almost all the times between hidden layers (not the output)\n",
    "\n",
    "*Tanh**: output between -1 and 1. Good because it's zero centered. Problem: saturate fast as sigmoid. Good in LSTM. \n",
    "\n",
    "![image](https://www.oreilly.com/library/view/machine-learning-with/9781789346565/assets/c9014c8e-7d06-4a12-9390-4d17f9379eb9.png)\n",
    "\n",
    "An activation function define how much an input neuron contribute to the next layer based on conditional threshold.\n",
    "\n",
    "It's a function that allows the neural network to approximate non-linear functions. In fact, it is also called non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-denial",
   "metadata": {},
   "source": [
    "**Softmax**\n",
    "\n",
    "A function that transforms an array of scores (the length of the array is the number of classes you want to predict) into probabilities.\n",
    "\n",
    "If our `criterion` (loss function) is `nn.NLLLoss()`, then we can use the `nn.LogSoftmax()` as output. This will compute the logarithm of the probabilities of each class.\n",
    "\n",
    "If we are using the nn.LogSoftmax(), and so we have the log probabilities, if we want to see the probability of each class we just compute the `torch.exp(logps)`.\n",
    "\n",
    "\n",
    "**Dropout**\n",
    "\n",
    "Used as regularization method to prevent overfitting. It turns off some neurons during training, and turn them on again in the validation part/inference part. It also speed up the training process because you have less computation to do (less weights to multiply).\n",
    "\n",
    "**Batch Normalization**\n",
    "\n",
    "Instead of just normalizing the inputs to the network, we normalize the inputs to layers within the network.\n",
    "\n",
    ">It's called batch normalization because during training, we normalize each layer's inputs by using the mean and variance of the values in the current batch.\n",
    "\n",
    "It makes the training faster and more stable through normalization of the layers' inputs by re-centering and re-scaling.\n",
    "\n",
    "Debates in \"why it works\".\n",
    "\n",
    "At the beginning, it makes the gradient to \"explode\", while *skip connections* (that are present in resnet for example) helps.\n",
    "\n",
    "In code:\n",
    "\n",
    "```nn.BatchNorm1d(hidden_dim)```\n",
    "\n",
    "`hidden_dim` must be the same dimension of the next layer input. So if before it there's a `Linear(input_dim, output_dim)`, we have that `hidden_dim=output_dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-question",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "based-career",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T07:39:36.961519Z",
     "start_time": "2021-06-04T07:39:36.890682Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "**Loss Function**\n",
    "\n",
    "The Loss function computes the error of our prediction.\n",
    "\n",
    "- Mean-Squared Error --> Used in Regression, `nn.MSELoss()`\n",
    "- NegativeLogLikelihood Loss --> Classification, in conjuction with `nn.LogSoftmax()`, in Pytorch `nn.NLLLoss()`\n",
    "- Cross Entropy Loss --> Classification, in Pytorch there's no need to add a softmax as output of the NN, `nn.CrossEntropyLoss()`.\n",
    "\n",
    "**Optimizer**\n",
    "\n",
    "It's an algorithm used to update the weights of our model in order to reduce the error given by the loss function. Some example of them are:\n",
    "\n",
    "- Stochastic Gradient Descent, SGD: updates the weights every batch. In Pytorch `optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9`\n",
    "\n",
    "- Adam Optimizer, it exploits many techniques to be more adaptive and use momentum in the updates.\n",
    "In Pytorch: `optimizer = optim.Adam(model.parameters(), lr=0.0001)`.\n",
    "\n",
    "... Other that you see on torch.optim Pytorch documentation\n",
    "\n",
    "**Learning Rate**\n",
    "\n",
    "An hyperparameter that we specify in the optimizer that define how much we update the weights at each iteration.\n",
    "\n",
    "An alternative definition is how much is the step size in the gradient descent process.\n",
    "\n",
    "**Some other definitions**:\n",
    "\n",
    "- *Logits*: the output of an hidden layer before applying the activation function (or a softmax)\n",
    "\n",
    "- Output probabilities: the probability of each class in the prediction of the NN. For binary classification problems, it's either a vector of two components (one hot encoding style - output of a 2-way softmax) or the output of a sigmoid (so single value). For multiclassification problem, let's say with N classes, it is the output of an N-way softmax.\n",
    "\n",
    "- Backpropagation: the algorithm that is used by neural networks to automatically compute the gradient of the loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-approval",
   "metadata": {},
   "source": [
    "**How a Neural Network learn**\n",
    "\n",
    "- A neural network learns from the data, that are the input of the network.\n",
    "- It computes a prediction as output (often as a probability, or a continuous value for regression)\n",
    "- The model consists of weights that are used to make the prediction\n",
    "- In a supervised learning approach, a \"loss function\" is used to compute the error for each prediction\n",
    "- To minimize the loss function, the gradient of it with respect to the weights of the model is computed using backpropagation\n",
    "- The weights are updated towards the steepest direction of the gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-joining",
   "metadata": {},
   "source": [
    "## What are the main categories of Deep Learning architectures?\n",
    "\n",
    "### Multilayer Perceptron or Fully Connected layers\n",
    "\n",
    "![image](https://www.researchgate.net/profile/Michael-Frish/publication/241347660/figure/fig3/AS:298690993508361@1448224890429/The-structure-of-a-multilayer-perceptron-neural-network.png)\n",
    "\n",
    "The basic component is the Perceptron, that is a Neuron.\n",
    "\n",
    "### Convolutional Neural Networks\n",
    "\n",
    "![image](https://miro.medium.com/max/3288/1*uAeANQIOQPqWZnnuH-VEyw.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-brush",
   "metadata": {},
   "source": [
    "### When do we use Convolutional Neural Networks?\n",
    "\n",
    "With *grid-like* data, like pictures, videos.\n",
    "\n",
    "Mainly when the absolute position of a feature is not relevant for the target prediction. \n",
    "\n",
    "**Why?** Because the convolution operation on the input makes it *translation invariant*.\n",
    "\n",
    "**What are other properties of CNNs?**\n",
    "\n",
    "*Parameter sharing*: the filters (that are learned during convolution) are shared all over the input.\n",
    "\n",
    "\n",
    "**What are the things that a CNN learns?** The filters.\n",
    "\n",
    "**How is a common CNN architecture defined?** We have Convolutational Layers, then Activations function (often *ReLu*), Pooling layers (no weights learned, often *Max Pooling*) for the \"feature extraction\" part. At the end, we have a \"classifier\", that is usually a MLP or just a fully connected layer with a softmax as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-investment",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-symposium",
   "metadata": {},
   "source": [
    "\n",
    "General rules for a good cheat sheet:\n",
    "\n",
    "- Definition\n",
    "- Applications\n",
    "- Code in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-input",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
