{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "psychological-alarm",
   "metadata": {},
   "source": [
    "## Importing Spacy\n",
    "\n",
    "SpaCy is a library that allows you to perform many NLP tasks, including preprocessing text by exploiting not only a rule based approach, but also some pretrained models, for example to split a text into sentences, or find the named entities contained in the text.\n",
    "\n",
    "To take advantage of it, you need to download it as explained in the following cell. For english, a common choice is `en_core_web_sm`, but also `en_core_web_lg` results in a good performance while being lightweight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unnecessary-pressing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:42.520625Z",
     "start_time": "2021-03-28T20:32:42.518920Z"
    }
   },
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "## I recommend the one above, because the following is more accurate but less efficient\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "frank-monaco",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:26:59.391180Z",
     "start_time": "2021-09-08T08:26:58.137087Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# You can also load en_core_web_lg that has an higher accuracy but it's less efficient\n",
    "# nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee85a78",
   "metadata": {},
   "source": [
    "By running `nlp.pipeline` you can see what are the steps that spaCy automatically does for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brutal-usage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:26:59.400494Z",
     "start_time": "2021-09-08T08:26:59.398316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x7fda6071ef10>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7fda5064b1c0>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7fda5064b100>)]\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b45ebf8",
   "metadata": {},
   "source": [
    "To use it, if you have loaded the model in a variable named `nlp` as above (`nlp = spacy.load(\"en_core_web_sm\")`), you can just pass the text you want as argument as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "marine-rescue",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:26:59.446450Z",
     "start_time": "2021-09-08T08:26:59.406845Z"
    }
   },
   "outputs": [],
   "source": [
    "# Process sentences 'Hello, world. Antonio is learning Python.' using spaCy\n",
    "doc = nlp(u\"Hello, world. Antonio is learning Python.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a935a5",
   "metadata": {},
   "source": [
    "By doing so, a lot of things happened! You can iterate over the \"doc\" and you will get the tokens of the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "behavioral-prospect",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:26:59.897610Z",
     "start_time": "2021-09-08T08:26:59.890403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      ",\n",
      "world\n",
      ".\n",
      "Antonio\n",
      "is\n",
      "learning\n",
      "Python\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fleet-shooting",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:27:02.666773Z",
     "start_time": "2021-09-08T08:27:02.662516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "# Get first token of the processed document\n",
    "token = doc[0]\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d5c82",
   "metadata": {},
   "source": [
    "The spaCy model automatically divide the text in sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2252cca3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:27:04.580743Z",
     "start_time": "2021-09-08T08:27:04.571436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world.\n",
      "Antonio is learning Python.\n"
     ]
    }
   ],
   "source": [
    "# Print sentences (one sentence per line)\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9eef52",
   "metadata": {},
   "source": [
    "It could look a trivial task, like it was a fancy way of doing `text.split(\".\")`. However, how would you split into sentences the text \"Im antonio im learning python\"?\n",
    "\n",
    "We know that \"im\" is a wrong way to write \"I'm\" and, since there are two verbs in that text, there are also two sentences. Let's see how spaCy performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b16de73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:27:40.775962Z",
     "start_time": "2021-09-08T08:27:40.753581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im antonio\n",
      "im learning python\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Im antonio im learning python\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef3ada5",
   "metadata": {},
   "source": [
    "Nice! However, don't get used to this, because there are tons of more complicated sentences that can easily be misinterpreted.\n",
    "\n",
    "In the example below, you can see that spaCy is smart enough to consider N.Y. as a single token, and not as two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "phantom-intranet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:47:57.191508Z",
     "start_time": "2021-09-08T08:47:57.058897Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = nlp(\"Let's go to N.Y.!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "designed-capitol",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:47:57.982864Z",
     "start_time": "2021-09-08T08:47:57.977939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let\n",
      "'s\n",
      "go\n",
      "to\n",
      "N.Y.\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-duplicate",
   "metadata": {},
   "source": [
    "As you have seen, using `nlp`, that comes from `spacy.load(\"en_core_web_sm\")`, you get the tokenized version of the sentence. If you want only the instance of the `Tokenizer` class, you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accomplished-marker",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.054850Z",
     "start_time": "2021-03-28T20:32:44.050350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokenizer.Tokenizer"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"tokenizer = nlp.tokenizer\\ntype(tokenizer)\";\n",
       "                var nbb_formatted_code = \"tokenizer = nlp.tokenizer\\ntype(tokenizer)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = nlp.tokenizer\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-medium",
   "metadata": {},
   "source": [
    "If you want to instantiate a custom one, with rules and prefixes and so on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "olive-mother",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.062007Z",
     "start_time": "2021-03-28T20:32:44.055855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"from spacy.tokenizer import Tokenizer\\n\\ntokenizer = Tokenizer(vocab=nlp.vocab)\";\n",
       "                var nbb_formatted_code = \"from spacy.tokenizer import Tokenizer\\n\\ntokenizer = Tokenizer(vocab=nlp.vocab)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(vocab=nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-clerk",
   "metadata": {},
   "source": [
    "The tokenizer defined above contains only english rules.\n",
    "Let's test it on \"Let's go to N.Y.!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "distinct-trade",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.068307Z",
     "start_time": "2021-03-28T20:32:44.063416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's\n",
      "go\n",
      "to\n",
      "N.Y.!\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"tokens = tokenizer(\\\"Let's go to N.Y.!\\\")\\nfor token in tokens:\\n    print(token)\";\n",
       "                var nbb_formatted_code = \"tokens = tokenizer(\\\"Let's go to N.Y.!\\\")\\nfor token in tokens:\\n    print(token)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens = tokenizer(\"Let's go to N.Y.!\")\n",
    "for token in tokens:\n",
    "    print(token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-county",
   "metadata": {},
   "source": [
    "As you can see here, it doesn't handle the exceptions about the dots. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-consumer",
   "metadata": {},
   "source": [
    "Looking at the output of `nlp.pipeline` above, we can see there are a tagger, a dependency parser and the entity recognizer. Let's check the entities of the following sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sufficient-radar",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:51:09.371036Z",
     "start_time": "2021-09-08T08:51:09.332008Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Apple is a $1000b company.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "mighty-twenty",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:51:09.736977Z",
     "start_time": "2021-09-08T08:51:09.731898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "a\n",
      "$\n",
      "1000b\n",
      "company\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "convenient-census",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.172302Z",
     "start_time": "2021-03-28T20:32:44.167426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "1000b MONEY\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"for ent in doc.ents:\\n    print(ent, ent.label_)\";\n",
       "                var nbb_formatted_code = \"for ent in doc.ents:\\n    print(ent, ent.label_)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-cassette",
   "metadata": {},
   "source": [
    "## Removing stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-brief",
   "metadata": {},
   "source": [
    "In general, it's convenient to remove all the stop words, *i.e. very common words in a language*, because they don't help most of NLP problem such as semantic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "going-palace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.290058Z",
     "start_time": "2021-03-28T20:32:44.283484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n",
      "First ten stop words: ['front', 'within', 'down', 'besides', 'each', 'until', 'more', 'take', 'have', 'various']\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 35;\n",
       "                var nbb_unformatted_code = \"spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\\nprint(\\\"Number of stop words: %d\\\" % len(spacy_stopwords))\\nprint(\\\"First ten stop words: %s\\\" % list(spacy_stopwords)[:10])\";\n",
       "                var nbb_formatted_code = \"spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\\nprint(\\\"Number of stop words: %d\\\" % len(spacy_stopwords))\\nprint(\\\"First ten stop words: %s\\\" % list(spacy_stopwords)[:10])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print(\"Number of stop words: %d\" % len(spacy_stopwords))\n",
    "print(\"First ten stop words: %s\" % list(spacy_stopwords)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-tokyo",
   "metadata": {},
   "source": [
    "To remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "crucial-entry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.317300Z",
     "start_time": "2021-03-28T20:32:44.291085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "determined\n",
      "drop\n",
      "his litigation\n",
      "the monastry\n",
      ",\n",
      "relinguish\n",
      "his claims\n",
      "wood\n",
      "-\n",
      "cuting\n",
      "\n",
      "\n",
      "fishery rihgts\n",
      ".\n",
      "ready\n",
      "this becuase\n",
      "the rights\n",
      "valuable\n",
      ",\n",
      "\n",
      "\n",
      "indeed the vaguest idea\n",
      "the wood\n",
      "river\n",
      "question\n",
      ".\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 36;\n",
       "                var nbb_unformatted_code = \"text = \\\"\\\"\\\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \\nfishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \\nindeed the vaguest idea where the wood and river in question were.\\\"\\\"\\\"\\n\\ndoc = nlp(text)\\n\\ntokens = [token.text for token in doc if not token.is_stop]\\nfor token in tokens:\\n    print(token)\";\n",
       "                var nbb_formatted_code = \"text = \\\"\\\"\\\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \\nfishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \\nindeed the vaguest idea where the wood and river in question were.\\\"\\\"\\\"\\n\\ndoc = nlp(text)\\n\\ntokens = [token.text for token in doc if not token.is_stop]\\nfor token in tokens:\\n    print(token)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in doc if not token.is_stop]\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-death",
   "metadata": {},
   "source": [
    "For adding customized stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "acknowledged-bahamas",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:30.546200Z",
     "start_time": "2021-03-28T20:33:30.528780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 39;\n",
       "                var nbb_unformatted_code = \"customize_stop_words = [\\\"computing\\\", \\\"filtered\\\"]\\nfor w in customize_stop_words:\\n    nlp.vocab[w].is_stop = True\";\n",
       "                var nbb_formatted_code = \"customize_stop_words = [\\\"computing\\\", \\\"filtered\\\"]\\nfor w in customize_stop_words:\\n    nlp.vocab[w].is_stop = True\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "customize_stop_words = [\"computing\", \"filtered\"]\n",
    "for w in customize_stop_words:\n",
    "    nlp.vocab[w].is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-scene",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "In most natural languages, a root word can have many variants. For example, the word ‘play’ can be used as ‘playing’, ‘played’, ‘plays’, etc. You can think of similar examples (and there are plenty).\n",
    "\n",
    "**Stemming**\n",
    "\n",
    "Let’s first understand stemming:\n",
    "\n",
    "Stemming is a text normalization technique that cuts off the end or beginning of a word by taking into account a list of common prefixes or suffixes that could be found in that word\n",
    "It is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word\n",
    " \n",
    "\n",
    "**Lemmatization**\n",
    "\n",
    "Lemmatization, on the other hand, is an organized & step-by-step procedure of obtaining the root form of the word. It makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations).\n",
    "\n",
    "Stemming algorithm works by cutting the suffix or prefix from the word. Lemmatization is a more powerful operation as it takes into consideration the morphological analysis of the word.\n",
    "\n",
    "Lemmatization returns the lemma, which is the root word of all its inflection forms.\n",
    "\n",
    "We can say that stemming is a quick and dirty method of chopping off words to its root form while on the other hand, lemmatization is an intelligent operation that uses dictionaries which are created by in-depth linguistic knowledge. Hence, Lemmatization helps in forming better features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "worthy-toilet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:36.947990Z",
     "start_time": "2021-03-28T20:33:36.465998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['determine',\n",
       " 'drop',\n",
       " 'litigation',\n",
       " 'monastry',\n",
       " ',',\n",
       " 'relinguish',\n",
       " 'claim',\n",
       " 'wood',\n",
       " '-',\n",
       " 'cut',\n",
       " '\\n',\n",
       " 'fishery',\n",
       " 'rihgts',\n",
       " '.',\n",
       " 'ready',\n",
       " 'becuase',\n",
       " 'right',\n",
       " 'valuable',\n",
       " ',',\n",
       " '\\n',\n",
       " 'vague',\n",
       " 'idea',\n",
       " 'wood',\n",
       " 'river',\n",
       " 'question',\n",
       " '.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 40;\n",
       "                var nbb_unformatted_code = \"nlp = spacy.load(\\\"en_core_web_sm\\\")\\nnlp.add_pipe(nlp.create_pipe(\\\"merge_entities\\\"))\\n# not using merge_chunk_nouns\\ndoc = nlp(\\n    u\\\"\\\"\\\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \\nfishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \\nindeed the vaguest idea where the wood and river in question were.\\\"\\\"\\\"\\n)\\n\\nlemma_word1 = []\\nfor token in doc:\\n    if token.is_stop:\\n        continue\\n    lemma_word1.append(token.lemma_)\\nlemma_word1\";\n",
       "                var nbb_formatted_code = \"nlp = spacy.load(\\\"en_core_web_sm\\\")\\nnlp.add_pipe(nlp.create_pipe(\\\"merge_entities\\\"))\\n# not using merge_chunk_nouns\\ndoc = nlp(\\n    u\\\"\\\"\\\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \\nfishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \\nindeed the vaguest idea where the wood and river in question were.\\\"\\\"\\\"\\n)\\n\\nlemma_word1 = []\\nfor token in doc:\\n    if token.is_stop:\\n        continue\\n    lemma_word1.append(token.lemma_)\\nlemma_word1\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(nlp.create_pipe(\"merge_entities\"))\n",
    "# not using merge_chunk_nouns\n",
    "doc = nlp(\n",
    "    u\"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    ")\n",
    "\n",
    "lemma_word1 = []\n",
    "for token in doc:\n",
    "    if token.is_stop:\n",
    "        continue\n",
    "    lemma_word1.append(token.lemma_)\n",
    "lemma_word1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-large",
   "metadata": {},
   "source": [
    "## Removing the punctuation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "tested-lafayette",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:38.236544Z",
     "start_time": "2021-03-28T20:33:38.220156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He determined to drop his litigation with the monastry and relinguish his claims to the woodcuting and \\nfishery rihgts at once He was the more ready to do this becuase the rights had become much less valuable and he had \\nindeed the vaguest idea where the wood and river in question were'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 41;\n",
       "                var nbb_unformatted_code = \"text = \\\"\\\"\\\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \\nfishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \\nindeed the vaguest idea where the wood and river in question were.\\\"\\\"\\\"\\n\\n\\nimport string\\n\\ntext_no_punct = \\\"\\\".join([char for char in text if char not in string.punctuation])\\n\\ntext_no_punct\";\n",
       "                var nbb_formatted_code = \"text = \\\"\\\"\\\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \\nfishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \\nindeed the vaguest idea where the wood and river in question were.\\\"\\\"\\\"\\n\\n\\nimport string\\n\\ntext_no_punct = \\\"\\\".join([char for char in text if char not in string.punctuation])\\n\\ntext_no_punct\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    "\n",
    "\n",
    "import string\n",
    "\n",
    "text_no_punct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "text_no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "institutional-commerce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:38.840465Z",
     "start_time": "2021-03-28T20:33:38.808474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He\n",
      "determined\n",
      "to\n",
      "drop\n",
      "his\n",
      "litigation\n",
      "with\n",
      "the\n",
      "monastry\n",
      "and\n",
      "relinguish\n",
      "his\n",
      "claims\n",
      "to\n",
      "the\n",
      "woodcuting\n",
      "and\n",
      "\n",
      "\n",
      "fishery\n",
      "rihgts\n",
      "at\n",
      "once\n",
      "He\n",
      "was\n",
      "the\n",
      "more\n",
      "ready\n",
      "to\n",
      "do\n",
      "this\n",
      "becuase\n",
      "the\n",
      "rights\n",
      "had\n",
      "become\n",
      "much\n",
      "less\n",
      "valuable\n",
      "and\n",
      "he\n",
      "had\n",
      "\n",
      "\n",
      "indeed\n",
      "the\n",
      "vaguest\n",
      "idea\n",
      "where\n",
      "the\n",
      "wood\n",
      "and\n",
      "river\n",
      "in\n",
      "question\n",
      "were\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 42;\n",
       "                var nbb_unformatted_code = \"doc = nlp(text_no_punct)\\nfor token in doc:\\n    print(token)\";\n",
       "                var nbb_formatted_code = \"doc = nlp(text_no_punct)\\nfor token in doc:\\n    print(token)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(text_no_punct)\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-source",
   "metadata": {},
   "source": [
    "For text extracted from dialogues or chats, it is convenient to preprocess the text so that multiple occurrences of the same characters get condensed into one or two, and then use a spell checker to find the correct form of the word.\n",
    "\n",
    "A way to do that is to replace all the occurrences of repeated characters with a single one and then use a spell checker: \"hhheeelllllooo hoooowww areee youuu?\" becomes \"helo how are you?\" and then the spell checker would make it \"hello how are you?\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "greater-cabin",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:44:46.506377Z",
     "start_time": "2021-03-28T20:44:46.492492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heLo how are you?'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 99;\n",
       "                var nbb_unformatted_code = \"st = \\\"hhheeeLLLLooo hoooowww areee youuu?????\\\"\\ntext = re.sub(r\\\"(.)\\\\1+\\\", r\\\"\\\\1\\\", st)\\ntext\";\n",
       "                var nbb_formatted_code = \"st = \\\"hhheeeLLLLooo hoooowww areee youuu?????\\\"\\ntext = re.sub(r\\\"(.)\\\\1+\\\", r\\\"\\\\1\\\", st)\\ntext\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "st = \"hhheeeLLLLooo hoooowww areee youuu?????\"\n",
    "text = re.sub(r\"(.)\\1+\", r\"\\1\", st)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "monthly-rapid",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:44:48.622525Z",
     "start_time": "2021-03-28T20:44:48.518501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 100;\n",
       "                var nbb_unformatted_code = \"from spellchecker import SpellChecker\\n\\ntext = nlp(text)\\nspell = SpellChecker()\\n\\n# find those words that may be misspelled\\nmisspelled = spell.unknown([token.text for token in text])\\n\\nfor word in misspelled:\\n    # Get the one `most likely` answer\\n    print(spell.correction(word))\\n\\n    # Get a list of `likely` options\\n    print(spell.candidates(word))\";\n",
       "                var nbb_formatted_code = \"from spellchecker import SpellChecker\\n\\ntext = nlp(text)\\nspell = SpellChecker()\\n\\n# find those words that may be misspelled\\nmisspelled = spell.unknown([token.text for token in text])\\n\\nfor word in misspelled:\\n    # Get the one `most likely` answer\\n    print(spell.correction(word))\\n\\n    # Get a list of `likely` options\\n    print(spell.candidates(word))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "text = nlp(text)\n",
    "spell = SpellChecker()\n",
    "\n",
    "# find those words that may be misspelled\n",
    "misspelled = spell.unknown([token.text for token in text])\n",
    "\n",
    "\n",
    "for word in misspelled:\n",
    "    # Get the one `most likely` answer\n",
    "    print(spell.correction(word))\n",
    "\n",
    "    # Get a list of `likely` options\n",
    "    print(spell.candidates(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-marriage",
   "metadata": {},
   "source": [
    "It didn't find any mispelled (even if there was \"helo\"). Try another spell checker:\n",
    "\n",
    "https://github.com/fsondej/autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "terminal-occupation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:43:37.953727Z",
     "start_time": "2021-03-28T20:43:37.889573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hero how are you?'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 95;\n",
       "                var nbb_unformatted_code = \"from autocorrect import Speller\\n\\nspell = Speller()\\n\\nspell(text.text)\";\n",
       "                var nbb_formatted_code = \"from autocorrect import Speller\\n\\nspell = Speller()\\n\\nspell(text.text)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "spell = Speller()\n",
    "\n",
    "spell(text.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-language",
   "metadata": {},
   "source": [
    "As you can see, it's not always working properly! However, overall it should improve your text.\n",
    "\n",
    "If you want to create a separate lemmatizer instead of having it in the pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-dressing",
   "metadata": {},
   "source": [
    "**For spacy before v3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "false-portrait",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T19:49:41.875502Z",
     "start_time": "2021-03-30T19:49:41.866457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['study']\n",
      "['studying']\n",
      "['studying']\n",
      "['study']\n",
      "['studying']\n",
      "['studying']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lemmatizer import Lemmatizer, ADJ, NOUN, VERB\n",
    "\n",
    "lemmatizer = nlp.vocab.morphology.lemmatizer\n",
    "print(lemmatizer(\"studying\", VERB))\n",
    "print(lemmatizer(\"studying\", NOUN))\n",
    "print(lemmatizer(\"studying\", ADJ))\n",
    "\n",
    "# or as alternative\n",
    "\n",
    "print(lemmatizer.verb(\"studying\"))\n",
    "print(lemmatizer.noun(\"studying\"))\n",
    "print(lemmatizer.adj(\"studying\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-header",
   "metadata": {},
   "source": [
    "spaCy has no built-in stemming! However, Lemmatization is enough for most of the tasks. As alternative, you can use [NLTK library](https://www.nltk.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-showcase",
   "metadata": {},
   "source": [
    "## Part of Speech (POS) Tagging\n",
    "\n",
    "Parts of speech tagging simply refers to assigning parts of speech to individual words in a sentence, which means that, unlike phrase matching, which is performed at the sentence or multi-word level, parts of speech tagging is performed at the token level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "valid-steering",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:36:29.855279Z",
     "start_time": "2021-03-31T04:36:29.828402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN\n",
      "AUX\n",
      "VERB\n",
      "PROPN\n",
      "ADP\n",
      "PROPN\n",
      "PROPN\n",
      "PUNCT\n"
     ]
    }
   ],
   "source": [
    "sentence = nlp(\"Antonio is learning Python in Strive School.\")\n",
    "\n",
    "for token in sentence:\n",
    "    print(token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-chair",
   "metadata": {},
   "source": [
    "The `.pos_` attribute gives the *coarse-grained* POS tag. To inspect the *fine-grained* POS tags we could use the `.tag_`attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "literary-beads",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:38:16.398804Z",
     "start_time": "2021-03-31T04:38:16.359022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP\n",
      "VBZ\n",
      "VBG\n",
      "NNP\n",
      "IN\n",
      "NNP\n",
      "NNP\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "sentence = nlp(\"Antonio is learning Python in Strive School.\")\n",
    "\n",
    "for token in sentence:\n",
    "    print(token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-arrow",
   "metadata": {},
   "source": [
    "While the output of the `.pos_` attribute is easy to decrypt (`PROPN`: proper noun,\n",
    "`AUX`: Auxiliary verb,\n",
    "`VERB`: verb,\n",
    "`ADP`: Adposition,\n",
    "`PUNCT`: Punctuation), the `.tag_`'s output is more cryptic. For this, you can use the `spacy.explain()` function to get the intuition behind that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "swiss-cement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:42:21.613532Z",
     "start_time": "2021-03-31T04:42:21.602635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun, proper singular\n",
      "verb, 3rd person singular present\n",
      "verb, gerund or present participle\n",
      "noun, proper singular\n",
      "conjunction, subordinating or preposition\n",
      "noun, proper singular\n",
      "noun, proper singular\n",
      "punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "for token in sentence:\n",
    "    print(spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-mercy",
   "metadata": {},
   "source": [
    "Go and dig up your primary school grammar book!\n",
    "\n",
    "Let's put everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "worst-bracelet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:45:40.533471Z",
     "start_time": "2021-03-31T04:45:40.515639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antonio      PROPN      NNP      noun, proper singular\n",
      "is           AUX        VBZ      verb, 3rd person singular present\n",
      "learning     VERB       VBG      verb, gerund or present participle\n",
      "Python       PROPN      NNP      noun, proper singular\n",
      "in           ADP        IN       conjunction, subordinating or preposition\n",
      "Strive       PROPN      NNP      noun, proper singular\n",
      "School       PROPN      NNP      noun, proper singular\n",
      ".            PUNCT      .        punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "for token in sentence:\n",
    "    print(f'{token.text:{12}} {token.pos_:{10}} {token.tag_:{8}} {spacy.explain(token.tag_)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-canberra",
   "metadata": {},
   "source": [
    "(the numbers between curly brackets define spaces for a better formatting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-greek",
   "metadata": {},
   "source": [
    "You can count the number of occurrences of each POS tag by calling the `count_by` method. \n",
    "\n",
    "The syntax is as follows (you need to pass `spacy.attrs.POS` as argument of the method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "valuable-cambodia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:01:14.881989Z",
     "start_time": "2021-03-31T05:01:14.735095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{96: 4, 87: 1, 100: 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = nlp(\"Antonio is learning Python Programming Language\")\n",
    "\n",
    "num_pos = sentence.count_by(spacy.attrs.POS)\n",
    "num_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-selection",
   "metadata": {},
   "source": [
    "The keys of the vocabulary are the ID of the POS tags, the values are their frequencies of occurrence. To retrieve the POS tags given the ID, you can do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "departmental-bermuda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:05:55.476564Z",
     "start_time": "2021-03-31T05:05:55.466354Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROPN'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.vocab[96].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-military",
   "metadata": {},
   "source": [
    "where 96 is the ID of the tag. Printing all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cultural-thursday",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:05:04.754457Z",
     "start_time": "2021-03-31T05:05:04.749748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 stands for PROPN   : 4\n",
      "87 stands for AUX     : 1\n",
      "100 stands for VERB    : 1\n"
     ]
    }
   ],
   "source": [
    "for ID, frequency in num_pos.items():\n",
    "    print(f\"{ID} stands for {sentence.vocab[ID].text:{8}}: {frequency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-chart",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "A named entity is a “real-world object” that’s assigned a name – for example, a person, a country, a product or a book title. spaCy can recognize various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn’t always work perfectly and might need some tuning later, depending on your use case.\n",
    "\n",
    "Named entities are available as the ents property of a Doc.\n",
    "\n",
    "\n",
    "Example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "pacific-generation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:04:34.612659Z",
     "start_time": "2021-03-29T06:04:34.586397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 129;\n",
       "                var nbb_unformatted_code = \"doc = nlp(\\\"Antonio works at Strive School.\\\")\";\n",
       "                var nbb_formatted_code = \"doc = nlp(\\\"Antonio works at Strive School.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"Antonio works at Strive School.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "prescription-swimming",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:04:49.092105Z",
     "start_time": "2021-03-29T06:04:49.075027Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Antonio works at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Strive School\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 131;\n",
       "                var nbb_unformatted_code = \"from spacy import displacy\\n\\ndisplacy.render(doc, style=\\\"ent\\\")\";\n",
       "                var nbb_formatted_code = \"from spacy import displacy\\n\\ndisplacy.render(doc, style=\\\"ent\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "substantial-bookmark",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:06:02.488436Z",
     "start_time": "2021-03-29T06:06:02.459044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 136;\n",
       "                var nbb_unformatted_code = \"doc = nlp(\\\"Rome is a big city.\\\")\";\n",
       "                var nbb_formatted_code = \"doc = nlp(\\\"Rome is a big city.\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"Rome is a big city.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "organized-reservoir",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:06:05.912053Z",
     "start_time": "2021-03-29T06:06:05.901532Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Rome\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " is a big city.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 137;\n",
       "                var nbb_unformatted_code = \"displacy.render(doc, style=\\\"ent\\\")\";\n",
       "                var nbb_formatted_code = \"displacy.render(doc, style=\\\"ent\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-gospel",
   "metadata": {},
   "source": [
    "ORG stands for organization, GPE stands for Geopolitical Entity. Some other tags are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-soldier",
   "metadata": {},
   "source": [
    "In spaCy you can list the entities by doing:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "removable-preserve",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:54:59.282842Z",
     "start_time": "2021-03-31T05:54:59.233747Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp('Manchester United is looking to sign Harry Kane for $90 million')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "sought-waste",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:54:59.773187Z",
     "start_time": "2021-03-31T05:54:59.766740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Manchester United, Harry Kane, $90 million)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-hospital",
   "metadata": {},
   "source": [
    "We can access the entities text, label by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "foreign-lincoln",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:56:13.150302Z",
     "start_time": "2021-03-31T05:56:13.141509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manchester United ORG\n",
      "Harry Kane PERSON\n",
      "$90 million MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-giant",
   "metadata": {},
   "source": [
    "Even if the entities are self-explanatory for this example, you can use `spacy.explain()` for a detailed description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "wound-nicaragua",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:57:32.686265Z",
     "start_time": "2021-03-31T05:57:32.678082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manchester United ORG Companies, agencies, institutions, etc.\n",
      "Harry Kane PERSON People, including fictional\n",
      "$90 million MONEY Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_, spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "prospective-springer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:11:28.227061Z",
     "start_time": "2021-03-31T06:11:28.210463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Strive\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is setting up a new course.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(sentence, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-trance",
   "metadata": {},
   "source": [
    "We can also filter which entity to display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "wound-scenario",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:12:13.012412Z",
     "start_time": "2021-03-31T06:12:12.922949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Manchester United\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking to sign \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Harry Kane\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $90 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    David\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " demand \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    100 Million Dollars\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = nlp(u'Manchester United is looking to sign Harry Kane for $90 million. David demand 100 Million Dollars')\n",
    "displacy.render(sentence, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "permanent-buffer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:12:25.772261Z",
     "start_time": "2021-03-31T06:12:25.766031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Manchester United\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking to sign Harry Kane for $90 million. David demand 100 Million Dollars</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filter = {'ents': ['ORG']}\n",
    "displacy.render(sentence, style='ent', jupyter=True, options=filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f01af1",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "To deal with text, it's often convenient to wrap all the preprocessing you want to do in a single function. Let's define one that:\n",
    "\n",
    "- split into tokens\n",
    "- remove stopwords\n",
    "- remove punctuation\n",
    "- make everything lowercase\n",
    "- lemmatize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5d0cd15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:31:18.079420Z",
     "start_time": "2021-09-08T13:31:18.054166Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing(sentence):\n",
    "    \"\"\"\n",
    "    params sentence: a str containing the sentence we want to preprocess\n",
    "    return the tokens list\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_stop]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c49f43f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:31:30.004823Z",
     "start_time": "2021-09-08T13:31:29.882100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentence', 'go', 'preprocess']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing(\"This is a sentence I'm going to preprocess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d4025b",
   "metadata": {},
   "source": [
    "As you can see, removing \"stopwords\" can be brutal, cause you will be missing a lot of important words that helps for the meaning of the sentence. For example, we missed the word \"I\". For this reason, sometimes is better to specify a list of words that you think won't be meaningful for the task you're going to perform. A good way to find them, is by checking the most frequent words in the corpus you have.\n",
    "\n",
    "Let's say you have this cell as text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b8f2d22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:35:11.460396Z",
     "start_time": "2021-09-08T13:35:11.456638Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"\"\"As you can see, removing \"stopwords\" can be brutal, cause you will be missing a lot of important words that helps for the meaning of the sentence. For example, we missed the word \"I\". For this reason, sometimes is better to specify a list of words that you think won't be meaningful for the task you're going to perform. A good way to find them, is by checking the most frequent words in the corpus you have.\n",
    "\n",
    "Let's say you have this cell as text:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5428b49f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d735ad4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:36:08.224301Z",
     "start_time": "2021-09-08T13:36:08.220013Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "counter.update(text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f74b2f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:36:42.531023Z",
     "start_time": "2021-09-08T13:36:42.517764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'As': 1,\n",
       "         'you': 5,\n",
       "         'can': 2,\n",
       "         'see,': 1,\n",
       "         'removing': 1,\n",
       "         '\"stopwords\"': 1,\n",
       "         'be': 3,\n",
       "         'brutal,': 1,\n",
       "         'cause': 1,\n",
       "         'will': 1,\n",
       "         'missing': 1,\n",
       "         'a': 2,\n",
       "         'lot': 1,\n",
       "         'of': 3,\n",
       "         'important': 1,\n",
       "         'words': 3,\n",
       "         'that': 2,\n",
       "         'helps': 1,\n",
       "         'for': 2,\n",
       "         'the': 6,\n",
       "         'meaning': 1,\n",
       "         'sentence.': 1,\n",
       "         'For': 2,\n",
       "         'example,': 1,\n",
       "         'we': 1,\n",
       "         'missed': 1,\n",
       "         'word': 1,\n",
       "         '\"I\".': 1,\n",
       "         'this': 2,\n",
       "         'reason,': 1,\n",
       "         'sometimes': 1,\n",
       "         'is': 2,\n",
       "         'better': 1,\n",
       "         'to': 3,\n",
       "         'specify': 1,\n",
       "         'list': 1,\n",
       "         'think': 1,\n",
       "         \"won't\": 1,\n",
       "         'meaningful': 1,\n",
       "         'task': 1,\n",
       "         \"you're\": 1,\n",
       "         'going': 1,\n",
       "         'perform.': 1,\n",
       "         'A': 1,\n",
       "         'good': 1,\n",
       "         'way': 1,\n",
       "         'find': 1,\n",
       "         'them,': 1,\n",
       "         'by': 1,\n",
       "         'checking': 1,\n",
       "         'most': 1,\n",
       "         'frequent': 1,\n",
       "         'in': 1,\n",
       "         'corpus': 1,\n",
       "         \"have.\\n\\nLet's\": 1,\n",
       "         'say': 1,\n",
       "         'have': 1,\n",
       "         'cell': 1,\n",
       "         'as': 1,\n",
       "         'text:': 1})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a05eb",
   "metadata": {},
   "source": [
    "By typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3586b516",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:38:44.806425Z",
     "start_time": "2021-09-08T13:38:44.799951Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 6),\n",
       " ('you', 5),\n",
       " ('be', 3),\n",
       " ('of', 3),\n",
       " ('words', 3),\n",
       " ('to', 3),\n",
       " ('can', 2),\n",
       " ('a', 2),\n",
       " ('that', 2),\n",
       " ('for', 2)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf40830c",
   "metadata": {},
   "source": [
    "we get the 10 most common words. Let's say that I'm trying to get the gist of the text, then I could say that I get rid of \"the\" and \"a\" words. \n",
    "\n",
    "In that case, my preprocessing function becomes something like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59334f61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:40:21.545018Z",
     "start_time": "2021-09-08T13:40:21.531682Z"
    }
   },
   "outputs": [],
   "source": [
    "STOPWORDS = [\"the\", \"a\"]\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    \"\"\"\n",
    "    params sentence: a str containing the sentence we want to preprocess\n",
    "    return the tokens list\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct and not token.lemma_ in STOPWORDS]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66c1e8a",
   "metadata": {},
   "source": [
    "where you see I replaced the control `not token.is_stop()` with `not token.lemma_ in STOPWORDS`.\n",
    "\n",
    "As usual, be aware of the task you are going to perform is very important to improve the chances of an high accuracy instead of blindly applying a set of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4f7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
