{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Importing Spacy\n",
    "\n",
    "SpaCy is a library that allows you to perform many NLP tasks, including preprocessing text by exploiting not only a rule based approach, but also some pretrained models, for example to split a text into sentences, or find the named entities contained in the text.\n",
    "\n",
    "To take advantage of it, you need to download it as explained in the following cell. For english, a common choice is `en_core_web_sm`, but also `en_core_web_lg` results in a good performance while being lightweight."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#!conda install -c conda-forge spacy\r\n",
    "# !python -m spacy download en_core_web_sm\r\n",
    "## I recommend the one above, because the following is more accurate but less efficient\r\n",
    "#Â !python -m spacy download en_core_web_lg"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:42.520625Z",
     "start_time": "2021-03-28T20:32:42.518920Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import spacy\r\n",
    "\r\n",
    "\r\n",
    "nlp = spacy.load(\"en_core_web_sm\")\r\n",
    "# You can also load en_core_web_lg that has an higher accuracy but it's less efficient\r\n",
    "#nlp = spacy.load(\"en_core_web_lg\")\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:26:59.391180Z",
     "start_time": "2021-09-08T08:26:58.137087Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By running `nlp.pipeline` you can see what are the steps that spaCy automatically does for you. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "print(nlp.pipeline)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x000001C491E47E00>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x000001C494D66C20>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x000001C494B75A60>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x000001C494D71B00>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x000001C494D5BD80>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x000001C494B75F40>)]\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:26:59.400494Z",
     "start_time": "2021-09-08T08:26:59.398316Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To use it, if you have loaded the model in a variable named `nlp` as above (`nlp = spacy.load(\"en_core_web_sm\")`), you can just pass the text you want as argument as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Process sentences 'Hello, world. Antonio is learning Python.' using spaCy\r\n",
    "doc = nlp(u\"Hello, world. Antonio is learning Python.\")"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:26:59.446450Z",
     "start_time": "2021-09-08T08:26:59.406845Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By doing so, a lot of things happened! You can iterate over the \"doc\" and you will get the tokens of the text:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "for token in doc:\r\n",
    "    print(token.text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hello\n",
      ",\n",
      "world\n",
      ".\n",
      "Antonio\n",
      "is\n",
      "learning\n",
      "Python\n",
      ".\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:26:59.897610Z",
     "start_time": "2021-09-08T08:26:59.890403Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Get first token of the processed document\r\n",
    "token = doc[1]\r\n",
    "print(token)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ",\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:27:02.666773Z",
     "start_time": "2021-09-08T08:27:02.662516Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The spaCy model automatically divide the text in sentences:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Print sentences (one sentence per line)\r\n",
    "for sent in doc.sents:\r\n",
    "    print(sent)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hello, world.\n",
      "Antonio is learning Python.\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:27:04.580743Z",
     "start_time": "2021-09-08T08:27:04.571436Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It could look a trivial task, like it was a fancy way of doing `text.split(\".\")`. However, how would you split into sentences the text \"Im antonio im learning python\"?\n",
    "\n",
    "We know that \"im\" is a wrong way to write \"I'm\" and, since there are two verbs in that text, there are also two sentences. Let's see how spaCy performs:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "doc = nlp(\"Im antonio im learning python\")\r\n",
    "for sentence in doc.sents:\r\n",
    "    print(sentence)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Im antonio im learning python\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:27:40.775962Z",
     "start_time": "2021-09-08T08:27:40.753581Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nice! However, don't get used to this, because there are tons of more complicated sentences that can easily be misinterpreted.\n",
    "\n",
    "In the example below, you can see that spaCy is smart enough to consider N.Y. as a single token, and not as two:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "tokens = nlp(\"Let's go to N.Y.!\")"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:47:57.191508Z",
     "start_time": "2021-09-08T08:47:57.058897Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "for token in tokens:\r\n",
    "    print(token.text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Let\n",
      "'s\n",
      "go\n",
      "to\n",
      "N.Y.\n",
      "!\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:47:57.982864Z",
     "start_time": "2021-09-08T08:47:57.977939Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you have seen, using `nlp`, that comes from `spacy.load(\"en_core_web_sm\")`, you get the tokenized version of the sentence. If you want only the instance of the `Tokenizer` class, you can run:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "tokenizer = nlp.tokenizer\r\n",
    "type(tokenizer)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "spacy.tokenizer.Tokenizer"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.054850Z",
     "start_time": "2021-03-28T20:32:44.050350Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you want to instantiate a custom one, with rules and prefixes and so on:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from spacy.tokenizer import Tokenizer\r\n",
    "\r\n",
    "tokenizer = Tokenizer(vocab=nlp.vocab)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.062007Z",
     "start_time": "2021-03-28T20:32:44.055855Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The tokenizer defined above contains only english rules.\n",
    "Let's test it on \"Let's go to N.Y.!\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "tokens = tokenizer(\"Let's go to N.Y.!\")\r\n",
    "for token in tokens:\r\n",
    "    print(token)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Let's\n",
      "go\n",
      "to\n",
      "N.Y.!\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.068307Z",
     "start_time": "2021-03-28T20:32:44.063416Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see here, it doesn't handle the exceptions about the dots. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking at the output of `nlp.pipeline` above, we can see there are a tagger, a dependency parser and the entity recognizer. Let's check the entities of the following sentence:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "doc = nlp(\"Apple is a $1000b company.\")\r\n"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:51:09.371036Z",
     "start_time": "2021-09-08T08:51:09.332008Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "for token in doc:\r\n",
    "    print(token)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Apple\n",
      "is\n",
      "a\n",
      "$\n",
      "1000b\n",
      "company\n",
      ".\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T08:51:09.736977Z",
     "start_time": "2021-09-08T08:51:09.731898Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "for ent in doc.ents:\r\n",
    "    print(ent, ent.label_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Apple ORG\n",
      "1000b MONEY\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.172302Z",
     "start_time": "2021-03-28T20:32:44.167426Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Removing stop words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In general, it's convenient to remove all the stop words, *i.e. very common words in a language*, because they don't help most of NLP problem such as semantic analysis."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\r\n",
    "print(\"Number of stop words: %d\" % len(spacy_stopwords))\r\n",
    "print(\"First ten stop words: %s\" % list(spacy_stopwords)[:10])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of stop words: 326\n",
      "First ten stop words: ['else', 'six', \"'ve\", 'ca', 'mine', 'fifteen', 'about', 'own', 'per', 'when']\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.290058Z",
     "start_time": "2021-03-28T20:32:44.283484Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To remove them:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \r\n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \r\n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\r\n",
    "\r\n",
    "doc = nlp(text)\r\n",
    "\r\n",
    "tokens = [token.text for token in doc if not token.is_stop]\r\n",
    "for token in tokens:\r\n",
    "    print(token)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "determined\n",
      "drop\n",
      "litigation\n",
      "monastry\n",
      ",\n",
      "relinguish\n",
      "claims\n",
      "wood\n",
      "-\n",
      "cuting\n",
      "\n",
      "\n",
      "fishery\n",
      "rihgts\n",
      ".\n",
      "ready\n",
      "becuase\n",
      "rights\n",
      "valuable\n",
      ",\n",
      "\n",
      "\n",
      "vaguest\n",
      "idea\n",
      "wood\n",
      "river\n",
      "question\n",
      ".\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.317300Z",
     "start_time": "2021-03-28T20:32:44.291085Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For adding customized stop words:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "customize_stop_words = [\"computing\", \"filtered\"]\r\n",
    "for w in customize_stop_words:\r\n",
    "    nlp.vocab[w].is_stop = True"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:30.546200Z",
     "start_time": "2021-03-28T20:33:30.528780Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "In most natural languages, a root word can have many variants. For example, the word âplayâ can be used as âplayingâ, âplayedâ, âplaysâ, etc. You can think of similar examples (and there are plenty).\n",
    "\n",
    "**Stemming**\n",
    "\n",
    "Letâs first understand stemming:\n",
    "\n",
    "Stemming is a text normalization technique that cuts off the end or beginning of a word by taking into account a list of common prefixes or suffixes that could be found in that word\n",
    "It is a rudimentary rule-based process of stripping the suffixes (âingâ, âlyâ, âesâ, âsâ etc) from a word\n",
    " \n",
    "\n",
    "**Lemmatization**\n",
    "\n",
    "Lemmatization, on the other hand, is an organized & step-by-step procedure of obtaining the root form of the word. It makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations).\n",
    "\n",
    "Stemming algorithm works by cutting the suffix or prefix from the word. Lemmatization is a more powerful operation as it takes into consideration the morphological analysis of the word.\n",
    "\n",
    "Lemmatization returns the lemma, which is the root word of all its inflection forms.\n",
    "\n",
    "We can say that stemming is a quick and dirty method of chopping off words to its root form while on the other hand, lemmatization is an intelligent operation that uses dictionaries which are created by in-depth linguistic knowledge. Hence, Lemmatization helps in forming better features."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\r\n",
    "nlp.add_pipe(nlp.create_pipe(\"merge_entities\"))\r\n",
    "# not using merge_chunk_nouns\r\n",
    "doc = nlp(\r\n",
    "    u\"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \r\n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \r\n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\r\n",
    ")\r\n",
    "\r\n",
    "lemma_word1 = []\r\n",
    "for token in doc:\r\n",
    "    if token.is_stop:\r\n",
    "        continue\r\n",
    "    lemma_word1.append(token.lemma_)\r\n",
    "lemma_word1"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "[E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <function merge_entities at 0x000001C494438E50> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26044/576600410.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en_core_web_sm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"merge_entities\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# not using merge_chunk_nouns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m doc = nlp(\n\u001b[0;32m      5\u001b[0m     u\"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[1;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[0mbad_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfactory_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE966\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbad_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mfactory_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <function merge_entities at 0x000001C494438E50> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:36.947990Z",
     "start_time": "2021-03-28T20:33:36.465998Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Removing the punctuation\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \r\n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \r\n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\r\n",
    "\r\n",
    "\r\n",
    "import string\r\n",
    "\r\n",
    "text_no_punct = \"\".join([char for char in text if char not in string.punctuation])\r\n",
    "\r\n",
    "text_no_punct"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'He determined to drop his litigation with the monastry and relinguish his claims to the woodcuting and \\nfishery rihgts at once He was the more ready to do this becuase the rights had become much less valuable and he had \\nindeed the vaguest idea where the wood and river in question were'"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:38.236544Z",
     "start_time": "2021-03-28T20:33:38.220156Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "doc = nlp(text_no_punct)\r\n",
    "for token in doc:\r\n",
    "    print(token)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "He\n",
      "determined\n",
      "to\n",
      "drop\n",
      "his\n",
      "litigation\n",
      "with\n",
      "the\n",
      "monastry\n",
      "and\n",
      "relinguish\n",
      "his\n",
      "claims\n",
      "to\n",
      "the\n",
      "woodcuting\n",
      "and\n",
      "\n",
      "\n",
      "fishery\n",
      "rihgts\n",
      "at\n",
      "once\n",
      "He\n",
      "was\n",
      "the\n",
      "more\n",
      "ready\n",
      "to\n",
      "do\n",
      "this\n",
      "becuase\n",
      "the\n",
      "rights\n",
      "had\n",
      "become\n",
      "much\n",
      "less\n",
      "valuable\n",
      "and\n",
      "he\n",
      "had\n",
      "\n",
      "\n",
      "indeed\n",
      "the\n",
      "vaguest\n",
      "idea\n",
      "where\n",
      "the\n",
      "wood\n",
      "and\n",
      "river\n",
      "in\n",
      "question\n",
      "were\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:38.840465Z",
     "start_time": "2021-03-28T20:33:38.808474Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For text extracted from dialogues or chats, it is convenient to preprocess the text so that multiple occurrences of the same characters get condensed into one or two, and then use a spell checker to find the correct form of the word.\n",
    "\n",
    "A way to do that is to replace all the occurrences of repeated characters with a single one and then use a spell checker: \"hhheeelllllooo hoooowww areee youuu?\" becomes \"helo how are you?\" and then the spell checker would make it \"hello how are you?\"\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "st = \"hhheeeLLLLooo hoooowww areee youuu?????\"\r\n",
    "text = re.sub(r\"(.)\\1+\", r\"\\1\", st)\r\n",
    "text"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'English' object has no attribute 'sub'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26044/2615394981.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"hhheeeLLLLooo hoooowww areee youuu?????\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"(.)\\1+\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mr\"\\1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'English' object has no attribute 'sub'"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:44:46.506377Z",
     "start_time": "2021-03-28T20:44:46.492492Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "from spellchecker import SpellChecker\r\n",
    "\r\n",
    "text = nlp(text)\r\n",
    "spell = SpellChecker()\r\n",
    "\r\n",
    "# find those words that may be misspelled\r\n",
    "misspelled = spell.unknown([token.text for token in text])\r\n",
    "\r\n",
    "\r\n",
    "for word in misspelled:\r\n",
    "    # Get the one `most likely` answer\r\n",
    "    print(spell.correction(word))\r\n",
    "\r\n",
    "    # Get a list of `likely` options\r\n",
    "    print(spell.candidates(word))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rights\n",
      "{'rights'}\n",
      "i\n",
      "{'e', 'o', 'i', 'u', 'a', 'y'}\n",
      "cutting\n",
      "{'outing', 'curing', 'citing', 'cuing', 'muting', 'cunting', 'cueing', 'puting', 'cutting', 'cting'}\n",
      "monastery\n",
      "{'monastary', 'monastery'}\n",
      "relinquish\n",
      "{'relinquish'}\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:44:48.622525Z",
     "start_time": "2021-03-28T20:44:48.518501Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It didn't find any mispelled (even if there was \"helo\"). Try another spell checker:\n",
    "\n",
    "https://github.com/fsondej/autocorrect"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "from autocorrect import Speller\r\n",
    "\r\n",
    "spell = Speller()\r\n",
    "\r\n",
    "spell(text.text)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'He determined to drop his litigation with the monastery, and relinquish his claims to the wood-citing and \\nfishery rights at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \\nindeed the guest idea where the wood and river in question were.'"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:43:37.953727Z",
     "start_time": "2021-03-28T20:43:37.889573Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, it's not always working properly! However, overall it should improve your text.\n",
    "\n",
    "If you want to create a separate lemmatizer instead of having it in the pipeline:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**For spacy before v3**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "from spacy.lemmatizer import Lemmatizer, ADJ, NOUN, VERB\r\n",
    "\r\n",
    "lemmatizer = nlp.vocab.morphology.lemmatizer\r\n",
    "print(lemmatizer(\"studying\", VERB))\r\n",
    "print(lemmatizer(\"studying\", NOUN))\r\n",
    "print(lemmatizer(\"studying\", ADJ))\r\n",
    "\r\n",
    "# or as alternative\r\n",
    "\r\n",
    "print(lemmatizer.verb(\"studying\"))\r\n",
    "print(lemmatizer.noun(\"studying\"))\r\n",
    "print(lemmatizer.adj(\"studying\"))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy.Lemmatizer'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26044/2899033289.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLemmatizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLemmatizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mADJ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNOUN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVERB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmorphology\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"studying\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVERB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"studying\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNOUN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy.Lemmatizer'"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T19:49:41.875502Z",
     "start_time": "2021-03-30T19:49:41.866457Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "import nltk\r\n",
    "nltk.download('wordnet')\r\n",
    "from nltk.stem import WordNetLemmatizer "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Wojtek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# Init the Wordnet Lemmatizer\r\n",
    "lemmatizer = WordNetLemmatizer()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "# Lemmatize Single Word\r\n",
    "print(lemmatizer.lemmatize(\"bats\"))\r\n",
    "\r\n",
    "print(lemmatizer.lemmatize(\"are\"))\r\n",
    "\r\n",
    "\r\n",
    "print(lemmatizer.lemmatize(\"feet\"))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bat\n",
      "are\n",
      "foot\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "spacy.explain(verb)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'verb' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26044/806296611.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'verb' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "spaCy has no built-in stemming! However, Lemmatization is enough for most of the tasks. As alternative, you can use [NLTK library](https://www.nltk.org)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part of Speech (POS) Tagging\n",
    "\n",
    "Parts of speech tagging simply refers to assigning parts of speech to individual words in a sentence, which means that, unlike phrase matching, which is performed at the sentence or multi-word level, parts of speech tagging is performed at the token level."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "sentence = nlp(\"Antonio is learning Python in Strive School.\")\r\n",
    "\r\n",
    "for token in sentence:\r\n",
    "    print(token.pos_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PROPN\n",
      "AUX\n",
      "VERB\n",
      "PROPN\n",
      "ADP\n",
      "PROPN\n",
      "PROPN\n",
      "PUNCT\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:36:29.855279Z",
     "start_time": "2021-03-31T04:36:29.828402Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `.pos_` attribute gives the *coarse-grained* POS tag. To inspect the *fine-grained* POS tags we could use the `.tag_`attribute:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "sentence = nlp(\"Antonio is learning Python in Strive School.\")\r\n",
    "\r\n",
    "for token in sentence:\r\n",
    "    print(token.tag_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NNP\n",
      "VBZ\n",
      "VBG\n",
      "NNP\n",
      "IN\n",
      "NNP\n",
      "NNP\n",
      ".\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:38:16.398804Z",
     "start_time": "2021-03-31T04:38:16.359022Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "While the output of the `.pos_` attribute is easy to decrypt (`PROPN`: proper noun,\n",
    "`AUX`: Auxiliary verb,\n",
    "`VERB`: verb,\n",
    "`ADP`: Adposition,\n",
    "`PUNCT`: Punctuation), the `.tag_`'s output is more cryptic. For this, you can use the `spacy.explain()` function to get the intuition behind that:\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "for token in sentence:\r\n",
    "    print(spacy.explain(token.tag_))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "noun, proper singular\n",
      "verb, 3rd person singular present\n",
      "verb, gerund or present participle\n",
      "noun, proper singular\n",
      "conjunction, subordinating or preposition\n",
      "noun, proper singular\n",
      "noun, proper singular\n",
      "punctuation mark, sentence closer\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:42:21.613532Z",
     "start_time": "2021-03-31T04:42:21.602635Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Go and dig up your primary school grammar book!\n",
    "\n",
    "Let's put everything together:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "for token in sentence:\r\n",
    "    print(f'{token.text:{12}} {token.pos_:{10}} {token.tag_:{8}} {spacy.explain(token.tag_)}')\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Antonio      PROPN      NNP      noun, proper singular\n",
      "is           AUX        VBZ      verb, 3rd person singular present\n",
      "learning     VERB       VBG      verb, gerund or present participle\n",
      "Python       PROPN      NNP      noun, proper singular\n",
      "in           ADP        IN       conjunction, subordinating or preposition\n",
      "Strive       PROPN      NNP      noun, proper singular\n",
      "School       PROPN      NNP      noun, proper singular\n",
      ".            PUNCT      .        punctuation mark, sentence closer\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:45:40.533471Z",
     "start_time": "2021-03-31T04:45:40.515639Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(the numbers between curly brackets define spaces for a better formatting)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can count the number of occurrences of each POS tag by calling the `count_by` method. \n",
    "\n",
    "The syntax is as follows (you need to pass `spacy.attrs.POS` as argument of the method):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "sentence = nlp(\"Antonio is learning Python Programming Language\")\r\n",
    "\r\n",
    "num_pos = sentence.count_by(spacy.attrs.POS)\r\n",
    "num_pos"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{96: 4, 87: 1, 100: 1}"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:01:14.881989Z",
     "start_time": "2021-03-31T05:01:14.735095Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The keys of the vocabulary are the ID of the POS tags, the values are their frequencies of occurrence. To retrieve the POS tags given the ID, you can do as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "sentence.vocab[96].text"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'PROPN'"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:05:55.476564Z",
     "start_time": "2021-03-31T05:05:55.466354Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "where 96 is the ID of the tag. Printing all together:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "for ID, frequency in num_pos.items():\r\n",
    "    print(f\"{ID} stands for {sentence.vocab[ID].text:{8}}: {frequency}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "96 stands for PROPN   : 4\n",
      "87 stands for AUX     : 1\n",
      "100 stands for VERB    : 1\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:05:04.754457Z",
     "start_time": "2021-03-31T05:05:04.749748Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "A named entity is a âreal-world objectâ thatâs assigned a name â for example, a person, a country, a product or a book title. spaCy can recognize various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesnât always work perfectly and might need some tuning later, depending on your use case.\n",
    "\n",
    "Named entities are available as the ents property of a Doc.\n",
    "\n",
    "\n",
    "Example:\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "doc = nlp(\"Antonio works at Strive School.\")"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:04:34.612659Z",
     "start_time": "2021-03-29T06:04:34.586397Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "from spacy import displacy\r\n",
    "\r\n",
    "displacy.render(doc, style=\"ent\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Antonio works at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Strive School\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:04:49.092105Z",
     "start_time": "2021-03-29T06:04:49.075027Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "doc = nlp(\"Rome is a big city.\")"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:06:02.488436Z",
     "start_time": "2021-03-29T06:06:02.459044Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Rome\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " is a big city.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:06:05.912053Z",
     "start_time": "2021-03-29T06:06:05.901532Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ORG stands for organization, GPE stands for Geopolitical Entity. Some other tags are:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In spaCy you can list the entities by doing:\n",
    "    "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "doc = nlp('Manchester United is looking to sign Harry Kane for $90 million')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:54:59.282842Z",
     "start_time": "2021-03-31T05:54:59.233747Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "doc.ents"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Manchester United, Harry Kane, $90 million)"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:54:59.773187Z",
     "start_time": "2021-03-31T05:54:59.766740Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can access the entities text, label by doing:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "for ent in doc.ents:\r\n",
    "    print(ent.text, ent.label_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Manchester United ORG\n",
      "Harry Kane PERSON\n",
      "$90 million MONEY\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:56:13.150302Z",
     "start_time": "2021-03-31T05:56:13.141509Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Even if the entities are self-explanatory for this example, you can use `spacy.explain()` for a detailed description."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "for ent in doc.ents:\r\n",
    "    print(ent.text, ent.label_, spacy.explain(ent.label_))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Manchester United ORG Companies, agencies, institutions, etc.\n",
      "Harry Kane PERSON People, including fictional\n",
      "$90 million MONEY Monetary values, including unit\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:57:32.686265Z",
     "start_time": "2021-03-31T05:57:32.678082Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "from spacy import displacy\r\n",
    "\r\n",
    "displacy.render(sentence, style=\"ent\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Antonio\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " is learning \n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Python Programming Language\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:11:28.227061Z",
     "start_time": "2021-03-31T06:11:28.210463Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also filter which entity to display:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "sentence = nlp(u'Manchester United is looking to sign Harry Kane for $90 million. David demand 100 Million Dollars')\r\n",
    "displacy.render(sentence, style='ent', jupyter=True)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Manchester United\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking to sign \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Harry Kane\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $90 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    David\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " demand \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    100 Million Dollars\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:12:13.012412Z",
     "start_time": "2021-03-31T06:12:12.922949Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "filter = {'ents': ['ORG']}\r\n",
    "displacy.render(sentence, style='ent', jupyter=True, options=filter)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Manchester United\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking to sign Harry Kane for $90 million. David demand 100 Million Dollars</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:12:25.772261Z",
     "start_time": "2021-03-31T06:12:25.766031Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "\n",
    "To deal with text, it's often convenient to wrap all the preprocessing you want to do in a single function. Let's define one that:\n",
    "\n",
    "- split into tokens\n",
    "- remove stopwords\n",
    "- remove punctuation\n",
    "- make everything lowercase\n",
    "- lemmatize it"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "def preprocessing(sentence):\r\n",
    "    \"\"\"\r\n",
    "    params sentence: a str containing the sentence we want to preprocess\r\n",
    "    return the tokens list\r\n",
    "    \"\"\"\r\n",
    "    doc = nlp(sentence)\r\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_stop]\r\n",
    "    return tokens"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:31:18.079420Z",
     "start_time": "2021-09-08T13:31:18.054166Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "preprocessing(\"This is a sentence I'm going to preprocess\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['sentence', 'go', 'preprocess']"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:31:30.004823Z",
     "start_time": "2021-09-08T13:31:29.882100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, removing \"stopwords\" can be brutal, cause you will be missing a lot of important words that helps for the meaning of the sentence. For example, we missed the word \"I\". For this reason, sometimes is better to specify a list of words that you think won't be meaningful for the task you're going to perform. A good way to find them, is by checking the most frequent words in the corpus you have.\n",
    "\n",
    "Let's say you have this cell as text:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "text = \"\"\"As you can see, removing \"stopwords\" can be brutal, cause you will be missing a lot of important words that helps for the meaning of the sentence. For example, we missed the word \"I\". For this reason, sometimes is better to specify a list of words that you think won't be meaningful for the task you're going to perform. A good way to find them, is by checking the most frequent words in the corpus you have.\r\n",
    "\r\n",
    "Let's say you have this cell as text:\"\"\""
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:35:11.460396Z",
     "start_time": "2021-09-08T13:35:11.456638Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "from collections import Counter\r\n",
    "\r\n",
    "counter = Counter()\r\n",
    "counter.update(text.split(\" \"))"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:36:08.224301Z",
     "start_time": "2021-09-08T13:36:08.220013Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "counter"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'As': 1,\n",
       "         'you': 5,\n",
       "         'can': 2,\n",
       "         'see,': 1,\n",
       "         'removing': 1,\n",
       "         '\"stopwords\"': 1,\n",
       "         'be': 3,\n",
       "         'brutal,': 1,\n",
       "         'cause': 1,\n",
       "         'will': 1,\n",
       "         'missing': 1,\n",
       "         'a': 2,\n",
       "         'lot': 1,\n",
       "         'of': 3,\n",
       "         'important': 1,\n",
       "         'words': 3,\n",
       "         'that': 2,\n",
       "         'helps': 1,\n",
       "         'for': 2,\n",
       "         'the': 6,\n",
       "         'meaning': 1,\n",
       "         'sentence.': 1,\n",
       "         'For': 2,\n",
       "         'example,': 1,\n",
       "         'we': 1,\n",
       "         'missed': 1,\n",
       "         'word': 1,\n",
       "         '\"I\".': 1,\n",
       "         'this': 2,\n",
       "         'reason,': 1,\n",
       "         'sometimes': 1,\n",
       "         'is': 2,\n",
       "         'better': 1,\n",
       "         'to': 3,\n",
       "         'specify': 1,\n",
       "         'list': 1,\n",
       "         'think': 1,\n",
       "         \"won't\": 1,\n",
       "         'meaningful': 1,\n",
       "         'task': 1,\n",
       "         \"you're\": 1,\n",
       "         'going': 1,\n",
       "         'perform.': 1,\n",
       "         'A': 1,\n",
       "         'good': 1,\n",
       "         'way': 1,\n",
       "         'find': 1,\n",
       "         'them,': 1,\n",
       "         'by': 1,\n",
       "         'checking': 1,\n",
       "         'most': 1,\n",
       "         'frequent': 1,\n",
       "         'in': 1,\n",
       "         'corpus': 1,\n",
       "         \"have.\\n\\nLet's\": 1,\n",
       "         'say': 1,\n",
       "         'have': 1,\n",
       "         'cell': 1,\n",
       "         'as': 1,\n",
       "         'text:': 1})"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:36:42.531023Z",
     "start_time": "2021-09-08T13:36:42.517764Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By typing:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "counter.most_common(10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('the', 6),\n",
       " ('you', 5),\n",
       " ('be', 3),\n",
       " ('of', 3),\n",
       " ('words', 3),\n",
       " ('to', 3),\n",
       " ('can', 2),\n",
       " ('a', 2),\n",
       " ('that', 2),\n",
       " ('for', 2)]"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:38:44.806425Z",
     "start_time": "2021-09-08T13:38:44.799951Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "we get the 10 most common words. Let's say that I'm trying to get the gist of the text, then I could say that I get rid of \"the\" and \"a\" words. \n",
    "\n",
    "In that case, my preprocessing function becomes something like:\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "STOPWORDS = [\"the\", \"a\"]\r\n",
    "\r\n",
    "def preprocessing(sentence):\r\n",
    "    \"\"\"\r\n",
    "    params sentence: a str containing the sentence we want to preprocess\r\n",
    "    return the tokens list\r\n",
    "    \"\"\"\r\n",
    "    doc = nlp(sentence)\r\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct and not token.lemma_ in STOPWORDS]\r\n",
    "    return tokens"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T13:40:21.545018Z",
     "start_time": "2021-09-08T13:40:21.531682Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "where you see I replaced the control `not token.is_stop()` with `not token.lemma_ in STOPWORDS`.\n",
    "\n",
    "As usual, be aware of the task you are going to perform is very important to improve the chances of an high accuracy instead of blindly applying a set of steps."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('nlp': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "interpreter": {
   "hash": "cf2ea029bfa82dea0dd2008720eb4dab9a9af23f748d757fd319e395d73232e4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}