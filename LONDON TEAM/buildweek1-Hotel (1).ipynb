{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import re\r\n",
    "import time\r\n",
    "import random\r\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. NAMES\r\n",
    "\r\n",
    "#Empty Lists for Placeholders\r\n",
    "names_list = []\r\n",
    "\r\n",
    "#Global Link for Iterations in each page in the list (Not the first to be used)\r\n",
    "global_link = 'https://www.yelp.co.uk'\r\n",
    "\r\n",
    "#Base Link to get our first soup, to parse the href. HREF will come from this url\r\n",
    "base_url = \"https://www.yelp.co.uk/search?cflt=hotelstravel&find_loc=London%2C%20GB&start={}\"\r\n",
    "\r\n",
    "#Open file to write .csv in\r\n",
    "file_name = 'London Hotels.csv'\r\n",
    "\r\n",
    "#iterations of each 10-items yelp page\r\n",
    "for page_identifier in range (0, 241, 10):\r\n",
    "    url_string = base_url.format(page_identifier)\r\n",
    "    current_url = requests.get(url_string)\r\n",
    "\r\n",
    "    #define the initial soup for our 10-items page\r\n",
    "    soup = BeautifulSoup(current_url.content, \"html.parser\")\r\n",
    "       \r\n",
    "    #After the big soup is made, create the first bowl (all data inside each element)\r\n",
    "    #OBSOLETE (All data is from subsoup)\r\n",
    "    #This is a worst case scenario if the information is only in page lists\r\n",
    "    shorter_parent_class_string = \"container__09f24__21w3G hoverable__09f24__2nTf3 margin-t3__09f24__5bM2Z margin-b3__09f24__1DQ9x padding-t3__09f24__-R_5x padding-r3__09f24__1pBFG padding-b3__09f24__1vW6j padding-l3__09f24__1yCJf border--top__09f24__8W8ca border--right__09f24__1u7Gt border--bottom__09f24__xdij8 border--left__09f24__rwKIa border-color--default__09f24__1eOdn\"\r\n",
    "    all_data = soup.find_all('span', class_=shorter_parent_class_string)\r\n",
    "\r\n",
    "    #smaller bowl of soup for easier href parsing\r\n",
    "    data_containing_href = soup.find_all('span', class_='css-1pxmz4g')\r\n",
    "        \r\n",
    "    #href parsing using double nested loop\r\n",
    "    for data in data_containing_href:\r\n",
    "        for link in data.find_all('a', href=True):\r\n",
    "            \r\n",
    "            time.sleep(random.randint(1,4))\r\n",
    "\r\n",
    "            #Now do everything inside this loop!\r\n",
    "            new_url=requests.get(global_link + link['href'])\r\n",
    "\r\n",
    "            #Create a sub soup from the new_url\r\n",
    "            subsoup = BeautifulSoup(new_url.content, 'html.parser')\r\n",
    "\r\n",
    "            #Define the 'Bowls' for each type of datafrom the second soup\r\n",
    "            bowl_containing_names = subsoup.find('div', class_=\"headingLight__373c0__2ci9X margin-b1__373c0__1khoT border-color--default__373c0__2oFDT\")\r\n",
    "            \r\n",
    "            #Parsing for each type of data\r\n",
    "            #1. finding hotels' names\r\n",
    "            try:\r\n",
    "                namefinder = bowl_containing_names.get_text()\r\n",
    "                names_list.append(namefinder)\r\n",
    "\r\n",
    "            except Exception as e:\r\n",
    "                names_list.append(np.nan)\r\n",
    "                 \r\n",
    "             \r\n",
    "\r\n",
    "    \r\n",
    "    \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporary Save 1\r\n",
    "df = pd.DataFrame('Names':names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. PRICES\r\n",
    "\r\n",
    "#Empty Lists for Placeholders\r\n",
    "prices_list = []\r\n",
    "\r\n",
    "url_lists = []\r\n",
    "\r\n",
    "#Global Link for Iterations in each page in the list (Not the first to be used)\r\n",
    "global_link = 'https://www.yelp.co.uk'\r\n",
    "\r\n",
    "#Base Link to get our first soup, to parse the href. HREF will come from this url\r\n",
    "base_url = \"https://www.yelp.co.uk/search?cflt=hotelstravel&find_loc=London%2C%20GB&start={}\"\r\n",
    "\r\n",
    "#Open file to write .csv in\r\n",
    "file_name = 'London Hotels.csv'\r\n",
    "\r\n",
    "#iterations of each 10-items yelp page\r\n",
    "for page_identifier in range (0, 241, 10):\r\n",
    "    url_string = base_url.format(page_identifier)\r\n",
    "    current_url = requests.get(url_string)\r\n",
    "\r\n",
    "    #define the initial soup for our 10-items page\r\n",
    "    soup = BeautifulSoup(current_url.content, \"html.parser\")\r\n",
    "       \r\n",
    "        #After the big soup is made, create the first bowl (all data inside each element)\r\n",
    "    shorter_parent_class_string = \"container__09f24__21w3G hoverable__09f24__2nTf3 margin-t3__09f24__5bM2Z margin-b3__09f24__1DQ9x padding-t3__09f24__-R_5x padding-r3__09f24__1pBFG padding-b3__09f24__1vW6j padding-l3__09f24__1yCJf border--top__09f24__8W8ca border--right__09f24__1u7Gt border--bottom__09f24__xdij8 border--left__09f24__rwKIa border-color--default__09f24__1eOdn\"\r\n",
    "    all_data = soup.find_all('span', class_=shorter_parent_class_string)\r\n",
    "\r\n",
    "        #smaller bowl of soup for easier href parsing\r\n",
    "    data_containing_href = soup.find_all('span', class_='css-1pxmz4g')\r\n",
    "        \r\n",
    "        #href parsing using double nested loop\r\n",
    "    for data in data_containing_href:\r\n",
    "        for link in data.find_all('a', href=True):\r\n",
    "            \r\n",
    "            time.sleep(random.randint(1,4))\r\n",
    "\r\n",
    "            #Now do everything inside this loop!\r\n",
    "            new_url=requests.get(global_link + link['href'])\r\n",
    "\r\n",
    "            #Create a sub soup from the new_url\r\n",
    "            subsoup = BeautifulSoup(new_url.content, 'html.parser')\r\n",
    "\r\n",
    "            #Define the 'Bowls' for each type of datafrom the second soup\r\n",
    "            bowl_containing_prices = subsoup.find_all('span', class_='css-1xxismk')\r\n",
    "            \r\n",
    "            #Parsing for each type of data\r\n",
    "            #2. finding hotel's prices\r\n",
    "            try:\r\n",
    "                for i in range(len(bowl_containing_prices)):\r\n",
    "                    if bowl_containing_prices[i].text[0] == 'Â£':\r\n",
    "                        pricefinder = bowl_containing_prices[i].text\r\n",
    "                        break\r\n",
    "                prices_list.append(pricefinder)\r\n",
    "                time.sleep(random.randint(1,3))\r\n",
    "            \r\n",
    "            except Exception as e:\r\n",
    "                prices_list.append(np.nan)\r\n",
    "\r\n",
    "            \r\n",
    "            \r\n",
    "             \r\n",
    "\r\n",
    "    \r\n",
    "    \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporary Save 2\r\n",
    "df['Prices']=prices_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. RATINGS\r\n",
    "\r\n",
    "#Empty Lists for Placeholders\r\n",
    "ratings_list = []\r\n",
    "\r\n",
    "url_lists = []\r\n",
    "\r\n",
    "#Global Link for Iterations in each page in the list (Not the first to be used)\r\n",
    "global_link = 'https://www.yelp.co.uk'\r\n",
    "\r\n",
    "#Base Link to get our first soup, to parse the href. HREF will come from this url\r\n",
    "base_url = \"https://www.yelp.co.uk/search?cflt=hotelstravel&find_loc=London%2C%20GB&start={}\"\r\n",
    "\r\n",
    "#Open file to write .csv in\r\n",
    "file_name = 'London Hotels.csv'\r\n",
    "\r\n",
    "#iterations of each 10-items yelp page\r\n",
    "for page_identifier in range (0, 241, 10):\r\n",
    "    url_string = base_url.format(page_identifier)\r\n",
    "    current_url = requests.get(url_string)\r\n",
    "\r\n",
    "    #define the initial soup for our 10-items page\r\n",
    "    soup = BeautifulSoup(current_url.content, \"html.parser\")\r\n",
    "       \r\n",
    "        #After the big soup is made, create the first bowl (all data inside each element)\r\n",
    "    shorter_parent_class_string = \"container__09f24__21w3G hoverable__09f24__2nTf3 margin-t3__09f24__5bM2Z margin-b3__09f24__1DQ9x padding-t3__09f24__-R_5x padding-r3__09f24__1pBFG padding-b3__09f24__1vW6j padding-l3__09f24__1yCJf border--top__09f24__8W8ca border--right__09f24__1u7Gt border--bottom__09f24__xdij8 border--left__09f24__rwKIa border-color--default__09f24__1eOdn\"\r\n",
    "    all_data = soup.find_all('span', class_=shorter_parent_class_string)\r\n",
    "\r\n",
    "        #smaller bowl of soup for easier href parsing\r\n",
    "    data_containing_href = soup.find_all('span', class_='css-1pxmz4g')\r\n",
    "        \r\n",
    "        #href parsing using double nested loop\r\n",
    "    for data in data_containing_href:\r\n",
    "        for link in data.find_all('a', href=True):\r\n",
    "            \r\n",
    "            time.sleep(random.randint(1,4))\r\n",
    "\r\n",
    "            #Now do everything inside this loop!\r\n",
    "            new_url=requests.get(global_link + link['href'])\r\n",
    "\r\n",
    "            #Create a sub soup from the new_url\r\n",
    "            subsoup = BeautifulSoup(new_url.content, 'html.parser')\r\n",
    "\r\n",
    "            #Define the 'Bowls' for each type of datafrom the second soup\r\n",
    "            bowl_containing_ratings = subsoup.select(\".overflow--hidden__373c0__2B0kz\")\r\n",
    "            \r\n",
    "            #Parsing for each type of data\r\n",
    "            #3. finding hotels' ratings\r\n",
    "            try:\r\n",
    "                ratingfinder = bowl_containing_ratings[0]\r\n",
    "                ratingfinder = ratingfinder['aria-label']\r\n",
    "                ratingfinder = ratingfinder.split(\" \")[0]\r\n",
    "                ratingfinder = float(ratingfinder)\r\n",
    "                ratings_list.append(ratingfinder)\r\n",
    "\r\n",
    "            except Exception as e:\r\n",
    "                ratings_list.append(np.nan)\r\n",
    "\r\n",
    "            \r\n",
    "             \r\n",
    "\r\n",
    "    \r\n",
    "    \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporary Save - 3\r\n",
    "df['Ratings'] = ratings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. NUMBER OF REVIEWS\r\n",
    "\r\n",
    "#Empty Lists for Placeholders\r\n",
    "numreviews_list = []\r\n",
    "\r\n",
    "url_lists = []\r\n",
    "\r\n",
    "#Global Link for Iterations in each page in the list (Not the first to be used)\r\n",
    "global_link = 'https://www.yelp.co.uk'\r\n",
    "\r\n",
    "#Base Link to get our first soup, to parse the href. HREF will come from this url\r\n",
    "base_url = \"https://www.yelp.co.uk/search?cflt=hotelstravel&find_loc=London%2C%20GB&start={}\"\r\n",
    "\r\n",
    "#Open file to write .csv in\r\n",
    "file_name = 'London Hotels.csv'\r\n",
    "\r\n",
    "#iterations of each 10-items yelp page\r\n",
    "for page_identifier in range (0, 241, 10):\r\n",
    "    url_string = base_url.format(page_identifier)\r\n",
    "    current_url = requests.get(url_string)\r\n",
    "\r\n",
    "    #define the initial soup for our 10-items page\r\n",
    "    soup = BeautifulSoup(current_url.content, \"html.parser\")\r\n",
    "       \r\n",
    "    #After the big soup is made, create the first bowl (all data inside each element)\r\n",
    "    shorter_parent_class_string = \"container__09f24__21w3G hoverable__09f24__2nTf3 margin-t3__09f24__5bM2Z margin-b3__09f24__1DQ9x padding-t3__09f24__-R_5x padding-r3__09f24__1pBFG padding-b3__09f24__1vW6j padding-l3__09f24__1yCJf border--top__09f24__8W8ca border--right__09f24__1u7Gt border--bottom__09f24__xdij8 border--left__09f24__rwKIa border-color--default__09f24__1eOdn\"\r\n",
    "    all_data = soup.find_all('span', class_=shorter_parent_class_string)\r\n",
    "\r\n",
    "    #smaller bowl of soup for easier href parsing\r\n",
    "    data_containing_href = soup.find_all('span', class_='css-1pxmz4g')\r\n",
    "        \r\n",
    "    #href parsing using double nested loop\r\n",
    "    for data in data_containing_href:\r\n",
    "        for link in data.find_all('a', href=True):\r\n",
    "            \r\n",
    "            time.sleep(random.randint(1,4))\r\n",
    "\r\n",
    "            #Now do everything inside this loop!\r\n",
    "            new_url=requests.get(global_link + link['href'])\r\n",
    "\r\n",
    "            #Create a sub soup from the new_url\r\n",
    "            subsoup = BeautifulSoup(new_url.content, 'html.parser')\r\n",
    "\r\n",
    "            #Define the 'Bowls' for each type of datafrom the second soup\r\n",
    "            bowl_containing_numratings = subsoup.find(\"span\", class_=\"css-bq71j2\")\r\n",
    "            \r\n",
    "            #Parsing for each type of data\r\n",
    "            #4. Number of Review\r\n",
    "            try:\r\n",
    "                numreviewfinder = bowl_containing_numratings.get_text()\r\n",
    "                numreviewfinder = numreviewfinder.split(\" \")[0]\r\n",
    "                numreviewfinder = float(numreviewfinder)\r\n",
    "                numreviews_list.append(numreviewfinder)\r\n",
    "\r\n",
    "            except Exception as e:\r\n",
    "                numreviews_list.append(np.nan)\r\n",
    "            \r\n",
    "            \r\n",
    "             \r\n",
    "\r\n",
    "    \r\n",
    "    \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporary Save 4\r\n",
    "df['Number of Reviews'] = numreviews_list\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Addresses\r\n",
    "\r\n",
    "#Empty Lists for Placeholders\r\n",
    "addresses_list = []\r\n",
    "\r\n",
    "url_lists = []\r\n",
    "\r\n",
    "#Global Link for Iterations in each page in the list (Not the first to be used)\r\n",
    "global_link = 'https://www.yelp.co.uk'\r\n",
    "\r\n",
    "#Base Link to get our first soup, to parse the href. HREF will come from this url\r\n",
    "base_url = \"https://www.yelp.co.uk/search?cflt=hotelstravel&find_loc=London%2C%20GB&start={}\"\r\n",
    "\r\n",
    "#Open file to write .csv in\r\n",
    "file_name = 'London Hotels.csv'\r\n",
    "\r\n",
    "#iterations of each 10-items yelp page\r\n",
    "for page_identifier in range (0, 241, 10):\r\n",
    "    url_string = base_url.format(page_identifier)\r\n",
    "    current_url = requests.get(url_string)\r\n",
    "\r\n",
    "    #define the initial soup for our 10-items page\r\n",
    "    soup = BeautifulSoup(current_url.content, \"html.parser\")\r\n",
    "       \r\n",
    "        #After the big soup is made, create the first bowl (all data inside each element)\r\n",
    "    shorter_parent_class_string = \"container__09f24__21w3G hoverable__09f24__2nTf3 margin-t3__09f24__5bM2Z margin-b3__09f24__1DQ9x padding-t3__09f24__-R_5x padding-r3__09f24__1pBFG padding-b3__09f24__1vW6j padding-l3__09f24__1yCJf border--top__09f24__8W8ca border--right__09f24__1u7Gt border--bottom__09f24__xdij8 border--left__09f24__rwKIa border-color--default__09f24__1eOdn\"\r\n",
    "    all_data = soup.find_all('span', class_=shorter_parent_class_string)\r\n",
    "\r\n",
    "        #smaller bowl of soup for easier href parsing\r\n",
    "    data_containing_href = soup.find_all('span', class_='css-1pxmz4g')\r\n",
    "        \r\n",
    "        #href parsing using double nested loop\r\n",
    "    for data in data_containing_href:\r\n",
    "        for link in data.find_all('a', href=True):\r\n",
    "            \r\n",
    "            time.sleep(random.randint(1,4))\r\n",
    "\r\n",
    "            #Now do everything inside this loop!\r\n",
    "            new_url=requests.get(global_link + link['href'])\r\n",
    "\r\n",
    "            #Create a sub soup from the new_url\r\n",
    "            subsoup = BeautifulSoup(new_url.content, 'html.parser')\r\n",
    "\r\n",
    "            #Define the 'Bowls' for each type of datafrom the second soup\r\n",
    "            bowl_containing_addresses = subsoup.find_all(\"p\", class_=\"css-8yg8ez\")\r\n",
    "            \r\n",
    "            #Parsing for each type of data\r\n",
    "            #2. finding hotel's prices\r\n",
    "            try:\r\n",
    "                addressfinder = bowl_containing_addresses[2].get_text()\r\n",
    "                addresses_list.append(addressfinder)\r\n",
    "            \r\n",
    "            except Exception as e:\r\n",
    "                addresses_list.append(np.nan)\r\n",
    "\r\n",
    "            \r\n",
    "            \r\n",
    "             \r\n",
    "\r\n",
    "    \r\n",
    "    \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporary Save - 5\r\n",
    "df['Address'] = addresses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUPER TEMPORARY - ONLY SAVE ADDRESSES\r\n",
    "\r\n",
    "df_addresses = pd.DataFrame({'Address':addresses_list})\r\n",
    "df_addresses\r\n",
    "\r\n",
    "df_addresses.to_csv('londonhotel_address.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'names_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-804ed567035e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# DATAFRAME\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Hotel Name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnames_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Number of Reviews'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnumreviews_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Price Range'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mprices_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Ratings'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mratings_list\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'names_list' is not defined"
     ]
    }
   ],
   "source": [
    "# DATAFRAME\r\n",
    "\r\n",
    "df2 = pd.DataFrame({'Hotel Name': names_list, 'Number of Reviews':numreviews_list, 'Price Range':prices_list, 'Ratings':ratings_list})\r\n",
    "df2\r\n",
    "\r\n",
    "df2.to_csv('londonhotel.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "850c152f8f84f3548ed5f09fd834180be13dced819bcdf283d955f3afcab8cf5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('myenv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}